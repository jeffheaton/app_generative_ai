{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_04_1_langchain_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 4: LangChain: Chat and Memory**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 4 Material\n",
    "\n",
    "* **Part 4.1: LangChain Conversations** [[Video]](https://www.youtube.com/watch?v=-wXT2RlzJec&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_1_langchain_chat.ipynb)\n",
    "* Part 4.2: Conversation Buffer Window Memory [[Video]](https://www.youtube.com/watch?v=G-l3T1Z9CHc&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_2_memory_buffer.ipynb)\n",
    "* Part 4.3: Chat with Summary and Fixed Window [[Video]](https://www.youtube.com/watch?v=z0iTmoEgn9U&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_3_summary.ipynb)\n",
    "* Part 4.4: Chat with Persistence, Rollback and Regeneration [[Video]](https://www.youtube.com/watch?v=7QEFjNE6wxs&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_4_persistence.ipynb)\n",
    "* Part 4.5: Automated Coder Application [[Video]](https://www.youtube.com/watch?v=pHcKXOMDZKU&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_5_coder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "b873be76-8354-4f8d-d3e0-e59c0f09faeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.100.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 4.1: LangChain Conversations\n",
    "\n",
    "Large language models (LLMs) facilitate interaction like human conversations. They are capable of referencing information shared earlier in the dialogue. In this module, we will explore managing an LLM's memory capabilities. Notably, LLMs need an inherent memory system beyond their immediate context buffer. Consequently, it falls upon external systems like LangChain to embed all previous conversational memory into each new prompt. To effectively remember past interactions, LangChain maintains a comprehensive transcript that accumulates over time. This transcript is reintroduced to the LLM with each exchange, which incorporates both user inputs and LLM responses. With each new prompt, the LLM is tasked to generate a suitable next response, thereby perpetuating the interactive process.\n",
    "\n",
    "## Creating a Chat Conversation\n",
    "\n",
    "The code snippet provides two functions designed to create a basic conversation utility for a Large Language Model (LLM). This is part of an effort to build foundational utilities before introducing more complex memory capabilities using LangChain classes in later sections.\n",
    "\n",
    "First, the necessary modules and classes are imported. HumanMessage and SystemMessage from langchain_core.messages are used to represent messages from a human user and system responses, respectively. The ChatPromptTemplate, HumanMessagePromptTemplate, and SystemMessagePromptTemplate from langchain_core.prompts.chat help format prompts for the chat. Additionally, ChatOpenAI from langchain_openai is used to interact with OpenAI's language models, and display_markdown from IPython.display allows for markdown rendering within IPython environments.\n",
    "\n",
    "The first function, begin_conversation, initializes a conversation with a system prompt, which is a predefined message declaring that the system's role is to assist (specified by the DEFAULT_SYSTEM variable). It creates an initial SystemMessage containing this prompt and returns a list containing this message, setting the stage for a conversation.\n",
    "\n",
    "The second function, converse, facilitates the interaction between the human user and the LLM. It takes an LLM instance, the ongoing conversation (a list of messages), and the user's prompt as inputs. It begins by appending the user's message (encapsulated as a HumanMessage) to the conversation. It then invokes the LLM to respond based on the entire conversation history up to that point. The LLM's response is captured and added back to the conversation. Finally, the content of the LLM's response is returned, providing the output for the user to see. This function enables a dynamic and continuous exchange between the user and the system, leveraging the LLM's capabilities to generate relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "DEFAULT_SYSTEM = \"You are a helpful assistant. Format answers with markdown.\"\n",
    "\n",
    "def begin_conversation(sys_prompt):\n",
    "  messages = [\n",
    "      SystemMessage(content=sys_prompt)\n",
    "  ]\n",
    "  return messages\n",
    "\n",
    "def converse(llm, conversation, prompt):\n",
    "  conversation.append(HumanMessage(content=prompt))\n",
    "  output = llm.invoke(conversation)\n",
    "  conversation.append(output)\n",
    "  return output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "The provided code builds upon the initial snippet to create a simple conversation where the Large Language Model (LLM) recalls the user's name through a series of exchanges.\n",
    "\n",
    "The conversation starts by initializing an instance of ChatOpenAI, a class from the LangChain library, with a specific model ('gpt-5-mini'). Several parameters are set for the LLM, including the temperature, which controls the randomness of the response, and n, which specifies the number of responses to generate (in this case, a single response).\n",
    "\n",
    "The function begin_conversation is called with the constant DEFAULT_SYSTEM as its argument. This function initializes the conversation with a system message stating, \"You are a helpful assistant.\" The conversation list, containing this initial system message, is created and will be used to track the entire conversation history.\n",
    "\n",
    "Next, the converse function is used to facilitate the dialogue between the user and the LLM. The user begins by sending a prompt: \"Hello, what is my name?\" This user's message is appended to the conversation history, and the LLM is invoked to generate a response based on the conversation so far. The LLM's response is added to the conversation and displayed using display_markdown to format the output appropriately.\n",
    "\n",
    "In response to realizing the LLM does not yet know the user's name, the user then provides their name with the statement, \"Oh sorry, my name is Jeff.\" This message is similarly processed: added to the conversation, and the LLM generates a new response acknowledging the name or continuing the conversation. Again, the response is displayed.\n",
    "\n",
    "Finally, the user asks again, \"What is my name?\" to test if the LLM recalls the name from earlier in the conversation. The same process ensues, with the LLM generating a response based on the updated conversation history that now includes the user's name.\n",
    "\n",
    "This sequence effectively demonstrates a simple use case where the conversation history maintained in the list enables the LLM to recall and utilize context from earlier exchanges, such as remembering the user's name. The ability to recall details like this is crucial for creating more engaging and personalized user interactions with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "8a748a43-cc49-4907-fd77-bdc69848c9db"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know your name — you haven't told me yet. If you'd like, tell me now (for example: \"My name is Alex\") and I'll use it during this conversation.  \n",
       "\n",
       "Note: I can't access your accounts or personal data outside of what you share here, and I don't retain personal information between sessions unless you tell me again."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeff! I'll remember your name for the rest of this conversation. How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'gpt-5-mini'\n",
    "\n",
    "# Initialize the OpenAI LLM with your API key\n",
    "llm = ChatOpenAI(\n",
    "  model=MODEL,\n",
    "  temperature= 0.3,\n",
    "  n= 1)\n",
    "\n",
    "conversation = begin_conversation(DEFAULT_SYSTEM)\n",
    "output = converse(llm, conversation, \"Hello, what is my name?\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(llm, conversation, \"Oh sorry, my name is Jeff.\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(llm, conversation, \"What is my name?\")\n",
    "display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulYUrfVcnlgF"
   },
   "source": [
    "## Conversing with the LLM in Markdown\n",
    "\n",
    "The function chat serves as a facilitator for conversation between a human user and a Large Language Model (LLM) and enhances the display of the chat responses using Markdown formatting. It is built upon the earlier provided code snippets that set up a basic conversational framework with an LLM.\n",
    "\n",
    "Markdown is a lightweight markup language with plain-text formatting syntax. It is designed to be converted into HTML or other formats while remaining easy to read and write in its raw form. This feature makes it popular for writing on the web, as users can create formatted text (like headers, lists, italics, and bold text) using simple and readable symbols.\n",
    "\n",
    "In the chat function, the process starts by printing the user's prompt prefixed with \"Human: \". This mimics a real chat interface, clearly delineating the messages that are input by the user. Then, the function calls converse, which was previously described, to process the prompt within the ongoing conversation with the LLM. This function appends the human message to the conversation, invokes the LLM to generate a response based on the conversation's context, and then appends this system-generated message back into the conversation history.\n",
    "\n",
    "After obtaining the LLM's response through converse, the function display_markdown is used to render this response. The raw=True parameter tells the IPython display function to treat the string as raw Markdown. By using Markdown formatting, responses can include enhanced textual features such as italics, bolding, and lists, which can make the output more readable and engaging.\n",
    "\n",
    "By formatting LLM responses in Markdown, we reinforce the natural, text-based communication style of the LLM. Given that LLMs, such as those provided by OpenAI, often format their responses in Markdown to leverage its text-enhancement capabilities, this approach ensures that the conversation utility outputs responses that utilize the full range of expressive possibilities offered by Markdown, enhancing the user interaction experience. This design choice aligns well with the Markdown capabilities inherently supported by many LLMs, ensuring that the responses are both visually appealing and functionally informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHL6Ik8IM2PA"
   },
   "outputs": [],
   "source": [
    "def chat(llm, conversation, prompt):\n",
    "  print(f\"Human: {prompt}\")\n",
    "  output = converse(llm, conversation, prompt)\n",
    "  display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvzsYl3Qplac"
   },
   "source": [
    "The provided code sequence demonstrates a conversation between a human user and a Large Language Model (LLM), making use of the chat function to interactively manage the conversation and display responses in Markdown format. This approach allows for a dynamic and contextually aware chat, while also enhancing the visual and structural presentation of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "PtLDak7TM_FU",
    "outputId": "eb84ef16-a0be-4450-b6ca-c4b94a84d1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I don't know your name — I don't have access to personal data unless you tell me. Would you like to tell me now? If so, how should I address you (full name, first name, nickname)?  \n",
       "\n",
       "Options:\n",
       "- Tell me your name and I'll use it in this conversation.  \n",
       "- Ask me to remember it for future chats (note: I can only remember if you enable a memory feature; otherwise I won't retain it).  \n",
       "- Stay anonymous — I can call you \"Friend\" or \"User.\"  \n",
       "\n",
       "Which do you prefer?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeff — I’ll call you that in this chat.\n",
       "\n",
       "Would you like me to remember your name for future conversations?  \n",
       "- If yes: enable the memory feature (in your settings) and I’ll keep it.  \n",
       "- If no: I won’t retain it beyond this session.\n",
       "\n",
       "Anything you’d like help with right now?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff.  \n",
       "\n",
       "Would you like me to remember it for future conversations?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Give me a table of the 5 most populus cities with population and country.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Do you mean the 5 most populous cities by \"city proper\" (administrative boundaries) or by \"metropolitan area\"/\"urban agglomeration\"? Also which year or data source should I use (most recent estimates, UN 2022/2023, World Population Review, etc.)?  \n",
       "\n",
       "If you want, I can proceed now with a default: city proper using the most recent commonly cited estimates (and include the year/source). Which do you prefer?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = begin_conversation(DEFAULT_SYSTEM)\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"Give me a table of the 5 most populus cities with population and country.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeY-9YSOno_t"
   },
   "source": [
    "## Constraining the Conversation with a System Prompt\n",
    "\n",
    "You can use the system prompt to constrain the conversation to a specific topic. Here, we provide a simple agent that will only discuss life insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NyGpLJmWjGNq",
    "outputId": "15b8e4bd-0040-419e-95ba-c0ba38b30f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**I don’t know your name.** I don’t have access to personal data or identifiers unless you tell me.\n",
       "\n",
       "I’m here to help with life insurance questions. If you’d like to be called something specific, tell me how you’d like to be addressed and then ask a life-insurance question (examples you can ask):\n",
       "\n",
       "- Differences between term and whole life insurance  \n",
       "- How much coverage you might need  \n",
       "- How beneficiaries and payouts work  \n",
       "- What affects premiums and how to lower them  \n",
       "- Riders, conversions, and policy options\n",
       "\n",
       "Which life insurance question can I help with?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Hi Jeff — nice to meet you.**  \n",
       "\n",
       "How can I help with life insurance today? Here are some things I can do — pick one or tell me more about your situation:\n",
       "\n",
       "- Explain differences between term, whole, universal, and variable life insurance  \n",
       "- Estimate how much coverage you might need (I can use your age, income, debts, dependents, mortgage, savings)  \n",
       "- Explain beneficiaries, payout timing, and tax treatment  \n",
       "- Describe what affects premiums and how to lower them (medical exams, tobacco use, occupation, hobbies)  \n",
       "- Review riders (accelerated death benefit, waiver of premium, child rider, etc.) and conversion options  \n",
       "- Help compare quotes or evaluate an existing policy\n",
       "\n",
       "If you want a personalized estimate, tell me your age, general health (smoker? major conditions?), marital/dependent status, annual income, and any big debts (mortgage, loans)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Your name is Jeff.**\n",
       "\n",
       "How can I help with life insurance today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is your favorite programming language?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**I can’t answer that.** I only discuss life insurance topics.\n",
       "\n",
       "How can I help with life insurance today? Examples:\n",
       "- Explain term vs. whole life insurance  \n",
       "- Estimate how much coverage you may need (tell me age, health, income, debts, dependents)  \n",
       "- Explain beneficiaries, payouts, and taxes  \n",
       "- Describe what affects premiums and ways to lower them  \n",
       "- Review common riders (accelerated death benefit, waiver of premium, etc.)\n",
       "\n",
       "Which would you like?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is the difference between a term and whole life policy?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Below is a clear comparison of term vs. whole life insurance to help you decide which fits your needs.\n",
       "\n",
       "What they are\n",
       "- Term life: Provides a death benefit for a fixed period (common terms: 10, 15, 20, 30 years). If you die during the term, the policy pays the benefit; if you outlive the term, the coverage stops (unless renewed or converted).\n",
       "- Whole life: A permanent policy that stays in force for your lifetime as long as premiums are paid. It provides a guaranteed death benefit plus a cash value component that grows over time.\n",
       "\n",
       "Key differences at a glance\n",
       "- Duration\n",
       "  - Term: Temporary coverage for a set number of years.\n",
       "  - Whole: Permanent coverage for life.\n",
       "- Premiums\n",
       "  - Term: Much lower initially; level for the term length, then usually rises sharply on renewal.\n",
       "  - Whole: Higher, usually level for life (can be structured different ways).\n",
       "- Cash value\n",
       "  - Term: No cash value.\n",
       "  - Whole: Accumulates cash value you can access via loans or withdrawals.\n",
       "- Purpose\n",
       "  - Term: Income replacement, mortgage protection, temporary needs.\n",
       "  - Whole: Lifetime protection, estate planning, tax-deferred savings component.\n",
       "- Cost\n",
       "  - Term: More affordable coverage per dollar of death benefit.\n",
       "  - Whole: Significantly more expensive for the same death benefit amount.\n",
       "\n",
       "Other important considerations\n",
       "- Conversions: Many term policies include a conversion option to convert to a permanent policy without new underwriting—useful if health changes.\n",
       "- Renewability: Term policies often allow renewal at term end, but premiums may be much higher based on age.\n",
       "- Cash value access: Whole life cash value can be borrowed against or withdrawn; loans accrue interest and unpaid loans reduce the death benefit.\n",
       "- Guarantees vs. variability: Whole life offers more guarantees (premiums, death benefit, cash-value growth in traditional whole life). Universal and variable life are other permanent types with different risk/crediting features.\n",
       "- Tax treatment: Death benefits are generally income-tax-free to beneficiaries. Cash value growth in permanent policies is tax-deferred; loans typically aren’t taxed unless policy lapses.\n",
       "- Dividends: Some whole life policies (participating) may pay dividends—not guaranteed but can increase cash value or be used to lower premiums.\n",
       "\n",
       "Pros & cons\n",
       "- Term life\n",
       "  - Pros: Affordable, simple, good for temporary large needs (young families, mortgage).\n",
       "  - Cons: No savings/cash value; coverage can become expensive as you age.\n",
       "- Whole life\n",
       "  - Pros: Lifetime coverage, forced savings/cash value, predictable premiums (in many policies), useful for estate planning.\n",
       "  - Cons: Much higher premiums; complexity; lower effective investment returns compared with some alternatives.\n",
       "\n",
       "Which is right for you?\n",
       "- Choose term if: You need affordable coverage to protect dependents during working years, pay off a mortgage, or cover college costs.\n",
       "- Choose whole (or another permanent policy) if: You need guaranteed lifetime coverage, want a tax-deferred cash-value accumulation, have estate-planning needs, or want to lock in coverage while young and healthy.\n",
       "\n",
       "Next steps\n",
       "- Estimate how much coverage you need and for how long (I can help with a needs calculation if you give age, income, debts, dependents, mortgage).\n",
       "- If cost is a concern, compare term quotes first; if you value lifelong coverage or cash value, get quotes on whole/other permanent types and compare illustrated cash-value projections and fees.\n",
       "- If you already have a term policy, check whether it’s convertible and the conversion deadline.\n",
       "\n",
       "Would you like a quick coverage-need estimate or sample quote comparison based on your age and situation?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = begin_conversation(\"\"\"\n",
    "You are a helpful agent to answer questions about life insurance. Do not talk\n",
    "about anything else with users. . Format answers with markdown.\"\"\")\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"What is your favorite programming language?\")\n",
    "chat(llm, conversation, \"What is the difference between a term and whole life policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TjZs_TRht1n"
   },
   "source": [
    "# Module 4 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 4](https://github.com/jeffheaton/app_generative_ai/blob/main/assignments/assignment_yourname_t81_559_class4.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YQmfHaGBRLp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
