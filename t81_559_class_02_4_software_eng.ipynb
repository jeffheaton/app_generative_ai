{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whjsJasuhstV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_02_4_software_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOZxlIMhstX"
      },
      "source": [
        "# T81-559: Applications of Generative Artificial Intelligence\n",
        "**Module 2: Code Generation**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Yov72PhstY"
      },
      "source": [
        "# Module 2 Material\n",
        "\n",
        "* Part 2.1: Prompting for Code Generation [[Video]](https://www.youtube.com/watch?v=HVId6kYKKgQ) [[Notebook]](t81_559_class_02_1_dev.ipynb)\n",
        "* Part 2.2: Handling Revision Prompts [[Video]](https://www.youtube.com/watch?v=APpV46tplXA) [[Notebook]](t81_559_class_02_2_multi_prompt.ipynb)\n",
        "* Part 2.3: Using a LLM to Help Debug [[Video]](https://www.youtube.com/watch?v=VPqSNb38QK0) [[Notebook]](t81_559_class_02_3_llm_debug.ipynb)\n",
        "* **Part 2.4: Tracking Prompts in Software Development** [[Video]](https://www.youtube.com/watch?v=oUFUuYfvXZU) [[Notebook]](t81_559_class_02_4_software_eng.ipynb)\n",
        "* Part 2.5: Limits of LLM Code Generation [[Video]](https://www.youtube.com/watch?v=dKtRI0LZSyY) [[Notebook]](t81_559_class_02_5_code_gen_limits.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcAUP0c3hstY"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running and maps Google Drive if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsI496h5hstZ",
        "outputId": "1e55b6e3-4871-421c-ee11-41799fcc2248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.30)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.99.9)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# OpenAI Secrets\n",
        "if COLAB:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Install needed libraries in CoLab\n",
        "if COLAB:\n",
        "    !pip install langchain langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC9A-LaYhsta"
      },
      "source": [
        "# 2.4: Tracking Prompts in Software Development\n",
        "\n",
        "Prompting will undoubtedly become an essential part of modern software engineering. Programmers will likely construct individual prompts to generate methods. In this part, we will see how to write a prompt that consistently produces a non-trivial image cropping function. We will also store the prompt with the function so we do not lose the prompt in the future. Additionally, we will use automated unit tests to ensure that the generated method initially does what we expect, and continues to do so in the future, even if the technique is regenerated.\n",
        "\n",
        "## Conversational Code Generation\n",
        "\n",
        "We will continue to use the conversational code generation function provided in Module 2.2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TMF-rtxgRAea"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from IPython.display import display_markdown\n",
        "\n",
        "MODEL = \"gpt-5-mini\"\n",
        "SYSTEM_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an\n",
        "AI to generate Python code. If you have notes about the code, place them before\n",
        "the code. Any notes about execution should follow the code. If you do mix any\n",
        "notes with the code, make them comments. Add proper comments to the code.\n",
        "Sort imports and follow PEP 8 formatting.\n",
        "\"\"\"\n",
        "\n",
        "_histories = {}\n",
        "def _get_history(session_id: str):\n",
        "    return _histories.setdefault(session_id, InMemoryChatMessageHistory())\n",
        "\n",
        "def _build_chain():\n",
        "    llm = ChatOpenAI(model=MODEL, temperature=0.0, n=1)\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", SYSTEM_TEMPLATE),\n",
        "        MessagesPlaceholder(\"history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    return prompt | llm | StrOutputParser()\n",
        "\n",
        "def start_conversation(session_id: str = \"default\"):\n",
        "    base = _build_chain()\n",
        "    with_history = RunnableWithMessageHistory(\n",
        "        base,\n",
        "        lambda sid: _get_history(sid),\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"history\",\n",
        "    )\n",
        "    # Return a runnable pre-bound to the session id\n",
        "    return with_history.with_config(configurable={\"session_id\": session_id})\n",
        "\n",
        "def generate_code(conversation, prompt: str):\n",
        "    print(\"Model response:\")\n",
        "    result = conversation.invoke({\"input\": prompt})\n",
        "    display_markdown(result, raw=True)\n",
        "    return result\n",
        "\n",
        "# Usage:\n",
        "# conversation = start_conversation()  # or start_conversation(\"my-session\")\n",
        "# generate_code(conversation, \"Write a function to compute Levenshtein distance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze2LsFCScSQW"
      },
      "source": [
        "## Prompts for Consistent Code Generation\n",
        "\n",
        "The code below shows the prompt I created to generate the clipping function. It is essential to specify very clearly what you want from the LLM. In this case, I made sure to specify:\n",
        "\n",
        "* The name of the function\n",
        "* The name of all arguments to that function\n",
        "* What to return\n",
        "* The exact algorithm, including how to handle negative values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6bkupDjIbni5",
        "outputId": "099f6f71-8d15-4a8d-a81f-0bbefd77b10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Notes:\n- The function below uses OpenCV (cv2) and NumPy. Ensure these are installed in your environment.\n- The code follows PEP 8 style, includes comments, and a detailed docstring with an example.\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import Tuple\n\n\ndef safe_clip(\n    cv2_image: np.ndarray,\n    x: int,\n    y: int,\n    width: int,\n    height: int,\n    background\n) -> Tuple[np.ndarray, int, int]:\n    \"\"\"\n    Clip a rectangular region from an OpenCV image while safely handling\n    out-of-bounds coordinates and filling any missing areas with a\n    background color.\n\n    Parameters\n    ----------\n    cv2_image : np.ndarray\n        Source OpenCV image (H x W or H x W x C). The image can be single\n        channel (grayscale) or multi-channel (e.g., BGR).\n    x : int\n        X-coordinate (column) of the top-left corner of the requested clip\n        relative to the source image. Can be negative.\n    y : int\n        Y-coordinate (row) of the top-left corner of the requested clip\n        relative to the source image. Can be negative.\n    width : int\n        Width of the clipping area in pixels. Must be positive.\n    height : int\n        Height of the clipping area in pixels. Must be positive.\n    background : int or tuple\n        Background color used to fill areas outside the source image.\n        If the image has multiple channels, provide a tuple with one value\n        per channel (e.g., (B, G, R)). A single scalar will be broadcasted\n        to all channels.\n\n    Returns\n    -------\n    tuple (np.ndarray, int, int)\n        - new_image: np.ndarray\n            The resulting clipped image of size (height, width) or\n            (height, width, C), filled with the background color where the\n            source image did not provide pixels.\n        - x_offset: int\n            How much the origin of the clipped image has shifted in X\n            relative to the original image. This is the amount the source\n            content was moved right within the returned image. For example,\n            if x was -10, x_offset will be 10.\n        - y_offset: int\n            How much the origin of the clipped image has shifted in Y\n            relative to the original image. This is the amount the source\n            content was moved down within the returned image.\n\n    Notes\n    -----\n    - The function adjusts coordinates and size to fit within the image\n      dimensions. Areas requested outside the source image are filled with\n      the provided background color.\n    - Negative x or y effectively expands the source coordinate origin so the\n      returned image contains the requested region with the original image\n      positioned appropriately inside.\n\n    Example\n    -------\n    >>> import numpy as np\n    >>> import cv2\n    >>> # Create a 100x100 blue BGR image\n    >>> src = np.full((100, 100, 3), (255, 0, 0), dtype=np.uint8)\n    >>> # Request a 50x50 region starting at x=-10, y=20, using gray background\n    >>> clipped, x_off, y_off = safe_clip(src, x=-10, y=20, width=50,\n    ...                                  height=50, background=(128, 128, 128))\n    >>> clipped.shape\n    (50, 50, 3)\n    >>> x_off, y_off\n    (10, 0)\n    >>> # The original content appears starting at column 10 in clipped.\n    \"\"\"\n    # Validate width and height\n    if width <= 0 or height <= 0:\n        raise ValueError(\"width and height must be positive integers\")\n\n    # Determine source image dimensions and number of channels\n    if cv2_image.ndim == 2:\n        src_h, src_w = cv2_image.shape\n        channels = 1\n    elif cv2_image.ndim == 3:\n        src_h, src_w, channels = cv2_image.shape\n    else:\n        raise ValueError(\"cv2_image must be a 2D or 3D numpy array\")\n\n    # Create output image filled with the background color\n    # Ensure background matches the number of channels\n    if channels == 1:\n        # Single channel: create 2D array\n        new_img = np.full((height, width), background, dtype=cv2_image.dtype)\n    else:\n        # Multi-channel: broadcast scalar background or validate tuple length\n        if np.isscalar(background):\n            bg = [background] * channels\n        else:\n            bg = list(background)\n            if len(bg) != channels:\n                raise ValueError(\n                    \"Length of background tuple must match number of image channels\"\n                )\n        new_img = np.full((height, width, channels), bg, dtype=cv2_image.dtype)\n\n    # Compute overlapping region between requested rectangle and source image\n    src_x1 = max(x, 0)\n    src_y1 = max(y, 0)\n    src_x2 = min(x + width, src_w)\n    src_y2 = min(y + height, src_h)\n\n    # If there is no overlap, return the background-filled image and offsets\n    if src_x1 >= src_x2 or src_y1 >= src_y2:\n        x_offset = -min(x, 0)\n        y_offset = -min(y, 0)\n        return new_img, x_offset, y_offset\n\n    # Destination coordinates in the new image where source pixels will be placed\n    dst_x1 = src_x1 - x\n    dst_y1 = src_y1 - y\n    dst_x2 = dst_x1 + (src_x2 - src_x1)\n    dst_y2 = dst_y1 + (src_y2 - src_y1)\n\n    # Copy the overlapping region from source into the destination image\n    if channels == 1:\n        new_img[dst_y1:dst_y2, dst_x1:dst_x2] = cv2_image[src_y1:src_y2, src_x1:src_x2]\n    else:\n        new_img[dst_y1:dst_y2, dst_x1:dst_x2, :] = cv2_image[\n            src_y1:src_y2, src_x1:src_x2, :\n        ]\n\n    # Offsets indicate the shift of the original requested origin inside new_img\n    x_offset = dst_x1\n    y_offset = dst_y1\n\n    return new_img, x_offset, y_offset\n```\n\nExecution notes:\n- Requires OpenCV (cv2) and NumPy installed (e.g., pip install opencv-python numpy).\n- The function accepts BGR images (default OpenCV color order). If you work with other\n  color orders, handle conversions before or after calling safe_clip."
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Notes:\\n- The function below uses OpenCV (cv2) and NumPy. Ensure these are installed in your environment.\\n- The code follows PEP 8 style, includes comments, and a detailed docstring with an example.\\n\\n```python\\nimport cv2\\nimport numpy as np\\nfrom typing import Tuple\\n\\n\\ndef safe_clip(\\n    cv2_image: np.ndarray,\\n    x: int,\\n    y: int,\\n    width: int,\\n    height: int,\\n    background\\n) -> Tuple[np.ndarray, int, int]:\\n    \"\"\"\\n    Clip a rectangular region from an OpenCV image while safely handling\\n    out-of-bounds coordinates and filling any missing areas with a\\n    background color.\\n\\n    Parameters\\n    ----------\\n    cv2_image : np.ndarray\\n        Source OpenCV image (H x W or H x W x C). The image can be single\\n        channel (grayscale) or multi-channel (e.g., BGR).\\n    x : int\\n        X-coordinate (column) of the top-left corner of the requested clip\\n        relative to the source image. Can be negative.\\n    y : int\\n        Y-coordinate (row) of the top-left corner of the requested clip\\n        relative to the source image. Can be negative.\\n    width : int\\n        Width of the clipping area in pixels. Must be positive.\\n    height : int\\n        Height of the clipping area in pixels. Must be positive.\\n    background : int or tuple\\n        Background color used to fill areas outside the source image.\\n        If the image has multiple channels, provide a tuple with one value\\n        per channel (e.g., (B, G, R)). A single scalar will be broadcasted\\n        to all channels.\\n\\n    Returns\\n    -------\\n    tuple (np.ndarray, int, int)\\n        - new_image: np.ndarray\\n            The resulting clipped image of size (height, width) or\\n            (height, width, C), filled with the background color where the\\n            source image did not provide pixels.\\n        - x_offset: int\\n            How much the origin of the clipped image has shifted in X\\n            relative to the original image. This is the amount the source\\n            content was moved right within the returned image. For example,\\n            if x was -10, x_offset will be 10.\\n        - y_offset: int\\n            How much the origin of the clipped image has shifted in Y\\n            relative to the original image. This is the amount the source\\n            content was moved down within the returned image.\\n\\n    Notes\\n    -----\\n    - The function adjusts coordinates and size to fit within the image\\n      dimensions. Areas requested outside the source image are filled with\\n      the provided background color.\\n    - Negative x or y effectively expands the source coordinate origin so the\\n      returned image contains the requested region with the original image\\n      positioned appropriately inside.\\n\\n    Example\\n    -------\\n    >>> import numpy as np\\n    >>> import cv2\\n    >>> # Create a 100x100 blue BGR image\\n    >>> src = np.full((100, 100, 3), (255, 0, 0), dtype=np.uint8)\\n    >>> # Request a 50x50 region starting at x=-10, y=20, using gray background\\n    >>> clipped, x_off, y_off = safe_clip(src, x=-10, y=20, width=50,\\n    ...                                  height=50, background=(128, 128, 128))\\n    >>> clipped.shape\\n    (50, 50, 3)\\n    >>> x_off, y_off\\n    (10, 0)\\n    >>> # The original content appears starting at column 10 in clipped.\\n    \"\"\"\\n    # Validate width and height\\n    if width <= 0 or height <= 0:\\n        raise ValueError(\"width and height must be positive integers\")\\n\\n    # Determine source image dimensions and number of channels\\n    if cv2_image.ndim == 2:\\n        src_h, src_w = cv2_image.shape\\n        channels = 1\\n    elif cv2_image.ndim == 3:\\n        src_h, src_w, channels = cv2_image.shape\\n    else:\\n        raise ValueError(\"cv2_image must be a 2D or 3D numpy array\")\\n\\n    # Create output image filled with the background color\\n    # Ensure background matches the number of channels\\n    if channels == 1:\\n        # Single channel: create 2D array\\n        new_img = np.full((height, width), background, dtype=cv2_image.dtype)\\n    else:\\n        # Multi-channel: broadcast scalar background or validate tuple length\\n        if np.isscalar(background):\\n            bg = [background] * channels\\n        else:\\n            bg = list(background)\\n            if len(bg) != channels:\\n                raise ValueError(\\n                    \"Length of background tuple must match number of image channels\"\\n                )\\n        new_img = np.full((height, width, channels), bg, dtype=cv2_image.dtype)\\n\\n    # Compute overlapping region between requested rectangle and source image\\n    src_x1 = max(x, 0)\\n    src_y1 = max(y, 0)\\n    src_x2 = min(x + width, src_w)\\n    src_y2 = min(y + height, src_h)\\n\\n    # If there is no overlap, return the background-filled image and offsets\\n    if src_x1 >= src_x2 or src_y1 >= src_y2:\\n        x_offset = -min(x, 0)\\n        y_offset = -min(y, 0)\\n        return new_img, x_offset, y_offset\\n\\n    # Destination coordinates in the new image where source pixels will be placed\\n    dst_x1 = src_x1 - x\\n    dst_y1 = src_y1 - y\\n    dst_x2 = dst_x1 + (src_x2 - src_x1)\\n    dst_y2 = dst_y1 + (src_y2 - src_y1)\\n\\n    # Copy the overlapping region from source into the destination image\\n    if channels == 1:\\n        new_img[dst_y1:dst_y2, dst_x1:dst_x2] = cv2_image[src_y1:src_y2, src_x1:src_x2]\\n    else:\\n        new_img[dst_y1:dst_y2, dst_x1:dst_x2, :] = cv2_image[\\n            src_y1:src_y2, src_x1:src_x2, :\\n        ]\\n\\n    # Offsets indicate the shift of the original requested origin inside new_img\\n    x_offset = dst_x1\\n    y_offset = dst_y1\\n\\n    return new_img, x_offset, y_offset\\n```\\n\\nExecution notes:\\n- Requires OpenCV (cv2) and NumPy installed (e.g., pip install opencv-python numpy).\\n- The function accepts BGR images (default OpenCV color order). If you work with other\\n  color orders, handle conversions before or after calling safe_clip.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "conversation = start_conversation()\n",
        "generate_code(conversation,\"\"\"\n",
        "Write a Python function named safe_clip using the PEP-8 style guide. The function\n",
        "should accept five parameters: cv2_image, x, y, width, height, and background.\n",
        "The imports should be sorted alphabetically.\n",
        "\n",
        "The purpose of this function is to clip a region from an OpenCV image while a\n",
        "djusting for boundaries and filling any areas missing from the original image\n",
        "boundaries with a specified color. Ensure that the code includes comments and a\n",
        "docstring explaining the functionality in detail.\n",
        "\n",
        "The function should:\n",
        "\n",
        "Calculate the dimensions of the clipping region and adjust them if they extend\n",
        "beyond the image boundaries. Negative x and y indicate that the source image\n",
        "must be expanded to accomodate the these coordinate that were outside the image.\n",
        "Create a new image of the specified size, filled with the background color.\n",
        "Place the clipped region into the newly created image at the appropriate location.\n",
        "Return a tuple containing the new image, along with x and y offsets indicating\n",
        "how much the origin of the clipped image has shifted relative to the original image.\n",
        "The return type should be specified in the function's docstring, which should\n",
        "also detail the types and purposes of each parameter. Additionally, mention that\n",
        "the function will adjust coordinates and size to fit within the image dimensions and\n",
        "fill missing areas with the specified background color.\n",
        "\n",
        "Include a detailed example in the docstring illustrating how the function is\n",
        "called and what it returns.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCmjBxIdcesJ"
      },
      "source": [
        "## Trying Out the Generated Code\n",
        "\n",
        "The code below shows the prompt I created to generate the clipping function. It is essential to specify very clearly what you want from the LLM. In this case, I made sure to specify:\n",
        "\n",
        "* The name of the function\n",
        "* The name of all arguments to that function\n",
        "* What to return\n",
        "* The exact algorithm, including how to handle negative values\n",
        "\n",
        "The following code shows my function; I also embed the prompt inside the source as a comment. This way, I can track changes to both the function and prompt as I check both in a source code repository like GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q42Lx21EcjTf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "## safe_clip was generated by the following prompt:\n",
        "# Write a Python function named safe_clip using the PEP-8 style guide. The function\n",
        "# should accept five parameters: cv2_image, x, y, width, height, and background.\n",
        "# The imports should be sorted alphabetically.\n",
        "#\n",
        "# The purpose of this function is to clip a region from an OpenCV image while a\n",
        "# djusting for boundaries and filling any areas missing from the original image\n",
        "# boundaries with a specified color. Ensure that the code includes comments and a\n",
        "# docstring explaining the functionality in detail.\n",
        "#\n",
        "# The function should:\n",
        "#\n",
        "# Calculate the dimensions of the clipping region and adjust them if they extend\n",
        "# beyond the image boundaries. Negative x and y indicate that the source image\n",
        "# must be expanded to accomodate the these coordinate that were outside the image.\n",
        "# Create a new image of the specified size, filled with the background color.\n",
        "# Place the clipped region into the newly created image at the appropriate location.\n",
        "# Return a tuple containing the new image, along with x and y offsets indicating\n",
        "# how much the origin of the clipped image has shifted relative to the original image.\n",
        "# The return type should be specified in the function's docstring, which should\n",
        "# also detail the types and purposes of each parameter. Additionally, mention that\n",
        "# the function will adjust coordinates and size to fit within the image dimensions and\n",
        "# fill missing areas with the specified background color.\n",
        "#\n",
        "# Include a detailed example in the docstring illustrating how the function is\n",
        "# called and what it returns.\n",
        "\n",
        "def safe_clip(cv2_image, x, y, width, height, background):\n",
        "    \"\"\"\n",
        "    Clips a region from an OpenCV image, adjusting for boundaries and filling missing areas.\n",
        "\n",
        "    Parameters:\n",
        "        cv2_image (numpy.ndarray): The source image from which to clip the region.\n",
        "        x (int): The x-coordinate of the top-left corner of the clipping region.\n",
        "        y (int): The y-coordinate of the top-left corner of the clipping region.\n",
        "        width (int): The width of the clipping region.\n",
        "        height (int): The height of the clipping region.\n",
        "        background (tuple): A tuple representing the BGR color (e.g., (255, 255, 255) for white) used to fill missing areas.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - new_image (numpy.ndarray): The new image with the clipped region and background.\n",
        "            - x_offset (int): The x-coordinate offset of the new image relative to the original image.\n",
        "            - y_offset (int): The y-coordinate offset of the new image relative to the original image.\n",
        "\n",
        "    Example:\n",
        "        >>> img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
        "        >>> clipped_img, x_off, y_off = safe_clip(img, -10, -10, 120, 120, (255, 0, 0))\n",
        "        >>> print(clipped_img.shape, x_off, y_off)\n",
        "        ((120, 120, 3), 10, 10)\n",
        "    \"\"\"\n",
        "    # Calculate effective x, y, width, and height considering image boundaries\n",
        "    x_eff = max(x, 0)\n",
        "    y_eff = max(y, 0)\n",
        "    width_eff = min(width, cv2_image.shape[1] - x_eff)\n",
        "    height_eff = min(height, cv2_image.shape[0] - y_eff)\n",
        "\n",
        "    # Create a new image filled with the background color\n",
        "    new_image = np.full((height, width, 3), background, dtype=cv2_image.dtype)\n",
        "\n",
        "    # Calculate offsets if the requested region is out of the original image bounds\n",
        "    x_offset = -min(x, 0)\n",
        "    y_offset = -min(y, 0)\n",
        "\n",
        "    # Place the clipped part of the original image into the new image\n",
        "    new_image[y_offset:y_offset + height_eff, x_offset:x_offset + width_eff] = \\\n",
        "        cv2_image[y_eff:y_eff + height_eff, x_eff:x_eff + width_eff]\n",
        "\n",
        "    return new_image, x_offset, y_offset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U3wWY3jv1it"
      },
      "source": [
        "Now, I will create an image of a grayscale and show how the function performs on three different types of crops. One of the critical features of this cropping function is that it might expand the original image if the upper-left corner or lower-right corner is off of the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "DcmhBIZQclni",
        "outputId": "32aefe4a-bd63-42a0-970a-384d15cb7c36"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a demo image (100x100) with a gradient and a solid square in the center\n",
        "image = np.zeros((100, 100, 3), dtype=np.uint8)\n",
        "cv2.rectangle(image, (30, 30), (70, 70), (255, 255, 255), -1)  # White square\n",
        "for i in range(100):\n",
        "    image[:, i] = i * 2.55  # Gradient from black to white\n",
        "\n",
        "# Define background color\n",
        "background_color = (0, 0, 255)  # Red background\n",
        "\n",
        "# Perform clipping operations\n",
        "clipped_image_within, _, _ = safe_clip(image, 20, 20, 60, 60, background_color)\n",
        "clipped_image_edge, _, _ = safe_clip(image, 50, 50, 100, 100, background_color)\n",
        "clipped_image_outside, _, _ = safe_clip(image, -10, -10, 120, 120, background_color)\n",
        "\n",
        "# Setup matplotlib plots\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "axs[0].set_title('Original Image')\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(cv2.cvtColor(clipped_image_within, cv2.COLOR_BGR2RGB))\n",
        "axs[1].set_title('Clipped Within Bounds')\n",
        "axs[1].axis('off')\n",
        "\n",
        "axs[2].imshow(cv2.cvtColor(clipped_image_edge, cv2.COLOR_BGR2RGB))\n",
        "axs[2].set_title('Clipped At Edge')\n",
        "axs[2].axis('off')\n",
        "\n",
        "axs[3].imshow(cv2.cvtColor(clipped_image_outside, cv2.COLOR_BGR2RGB))\n",
        "axs[3].set_title('Clipped Outside Bounds')\n",
        "axs[3].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5g--vkll2de"
      },
      "source": [
        "## Provide Unit Tests\n",
        "\n",
        "Unit testing is an essential facet of software engineering. Unit testing is necessary when a prompt generates your code. The following code shows how I converted the previous three examples into unit tests.\n",
        "\n",
        "Most software architects suggest you should not use an LLM to generate the unit tests because you must know what is being tested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PFT-XSZ-l5U7"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class TestSafeClip(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.image = np.zeros((100, 100, 3), dtype=np.uint8)\n",
        "        cv2.rectangle(self.image, (30, 30), (70, 70), (255, 255, 255), -1)\n",
        "        for i in range(100):\n",
        "            self.image[:, i] = i * 2.55  # Gradient from black to white\n",
        "        self.background_color = (0, 0, 255)  # Blue background\n",
        "\n",
        "    def test_clip_within_bounds(self):\n",
        "        clipped_image, x_off, y_off = safe_clip(self.image, 20, 20, 60, 60, self.background_color)\n",
        "        self.assertEqual(clipped_image.shape, (60, 60, 3))\n",
        "        self.assertEqual((x_off, y_off), (0, 0))\n",
        "        self.assertTrue((clipped_image[0, 0] != self.background_color).all())\n",
        "\n",
        "    def test_clip_at_edge(self):\n",
        "        clipped_image, x_off, y_off = safe_clip(self.image, 50, 50, 100, 100, self.background_color)\n",
        "        self.assertEqual(clipped_image.shape, (100, 100, 3))\n",
        "        self.assertEqual((x_off, y_off), (0, 0))\n",
        "        self.assertTrue((clipped_image[0, 0] == [127, 127, 127]).all())\n",
        "\n",
        "    def test_clip_outside_bounds(self):\n",
        "        clipped_image, x_off, y_off = safe_clip(self.image, -10, -10, 120, 120, self.background_color)\n",
        "        self.assertEqual(clipped_image.shape, (120, 120, 3))\n",
        "        self.assertEqual((x_off, y_off), (10, 10))\n",
        "        self.assertTrue((clipped_image[10, 10] == [0, 0, 0]).all())\n",
        "\n",
        "test = TestSafeClip()\n",
        "test.setUp()\n",
        "test.test_clip_within_bounds()\n",
        "test.test_clip_at_edge()\n",
        "test.test_clip_outside_bounds()"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (genai)",
      "language": "python",
      "name": "genai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}