{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_04_4_persistence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 4: LangChain: Chat and Memory**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 4 Material\n",
    "\n",
    "* Part 4.1: LangChain Conversations [[Video]]() [[Notebook]](t81_559_class_04_1_langchain_chat.ipynb)\n",
    "* Part 4.2: Conversation Buffer Window Memory [[Video]]() [[Notebook]](t81_559_class_04_2_memory_buffer.ipynb)\n",
    "* Part 4.3: Chat with Summary and Fixed Window [[Video]]() [[Notebook]](t81_559_class_04_3_summary.ipynb)\n",
    "* **Part 4.4: Chat with Persistence, Rollback and Regeneration** [[Video]]() [[Notebook]](t81_559_class_04_4_persistence.ipynb)\n",
    "* Part 4.5: Automated Coder Application [[Video]]() [[Notebook]](t81_559_class_04_5_coder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "1721abb9-1940-4c05-ed59-807c35588b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.100.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai\n",
    "    !wget -q https://raw.githubusercontent.com/jeffheaton/app_generative_ai/main/util_chat.py -O util_chat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 4.4: Persistence, Rollback and Regeneration\n",
    "\n",
    "## Introducing the ChatConversation Class  \n",
    "\n",
    "In this module we introduce the `ChatConversation` class, which builds on the foundation of the earlier `SimpleConversation` wrapper. The original version provided a straightforward way to send prompts, display responses, and track conversation history. While useful for lightweight experimentation, it was limited in how history was managed and offered few tools for editing or persisting sessions. The new `ChatConversation` class expands this design into a more flexible and production-ready framework.  \n",
    "\n",
    "The key innovation of `ChatConversation` is its use of pluggable message strategies. Instead of always storing the full conversation transcript, you can now choose from different strategies such as a passthrough mode that keeps everything, a fixed window that trims to the most recent messages, or a summarization strategy that compresses older content into a running summary. Each strategy is modular, registered in a common base class, and can be swapped in or out as needed. This provides a clean way to experiment with different approaches to memory without altering the main conversation logic.  \n",
    "\n",
    "Another major enhancement is the addition of editing controls. With `ChatConversation`, you are no longer locked into a strict linear progression of user and AI turns. You can undo the most recent interaction or regenerate an answer to a prior input. These tools make it easier to recover from mistakes, test variations of outputs, and maintain a smooth conversation flow. Logging is integrated into these operations to provide transparency and traceability.  \n",
    "\n",
    "Finally, `ChatConversation` introduces the ability to persist and restore conversations. You can save a session’s state to disk, capturing both the history and the active strategy configuration. Later, you can reload the conversation and resume where you left off. This persistence feature is especially valuable for experiments, reproducibility, and applications that need continuity across sessions. Together, these improvements make `ChatConversation` a powerful and adaptable tool for managing conversational AI workflows.  \n",
    "\n",
    "For convenience, we place the `ChatConversation` class and its supporting strategies into a separate Python file named `util_chat.py`. This allows us to keep the module organized and makes it easy to reuse the functionality in notebooks or other projects. Once saved, you can simply bring it into your workflow with:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "import util_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "We can now carry on a simple conversation with the LLM, using LangChain to track the conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Hc_VyjO1PT4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cem9-FrVyBaM"
   },
   "source": [
    "We will start by looking at a few examples that demonstrate how the new class works with different memory strategies. In particular, we will compare the buffer approach, which keeps the most recent turns verbatim, with the summary approach, which compresses older context into a running synopsis. These examples will highlight how each strategy affects what the model remembers and how the conversation evolves over time.  \n",
    "\n",
    "### Example: Fixed-window “buffer” memory\n",
    "\n",
    "The following example demonstrates how to use the fixed-window strategy to preserve memory across sessions. We begin by starting a conversation, providing some context, and saving the state to disk. Next, we create a new session with a different configuration and load the saved memory. Finally, we verify that the model recalls the previously stored fact, showing how persistence works with the buffer approach.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "44Q3DK6hybYa",
    "outputId": "b2f2e9f3-05e6-4334-dab9-eb4661a90b07"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hi, my name is Jeff."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hi Jeff — nice to meet you. How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember my favorite color is blue."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Got it, Jeff — I’ll remember that your favorite color is blue for the rest of this conversation. Would you like me to use that now (for example, when suggesting colors or examples), or save any other preferences?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What is my favorite color?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Your favorite color is blue. Would you like me to remember that across future conversations too?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Fixed-window (buffer) example ---\n",
    "# Assumes the ChatConversation and strategies code from before is already defined in the notebook.\n",
    "\n",
    "# 1) Start a session, teach a fact, and save memory\n",
    "conv_fixed_1 = util_chat.ChatConversation(\n",
    "    strategy_name=\"fixed\",\n",
    "    strategy_kwargs={\"window\": 14, \"keep_system_first\": True},\n",
    ")\n",
    "conv_fixed_1.chat(\"Hi, my name is Jeff.\")\n",
    "conv_fixed_1.chat(\"Please remember my favorite color is blue.\")\n",
    "conv_fixed_1.save(\"mem_fixed.json\")\n",
    "\n",
    "# 2) New session (could be a new process), load memory, and verify it works\n",
    "conv_fixed_2 = util_chat.ChatConversation(\n",
    "    strategy_name=\"fixed\",\n",
    "    strategy_kwargs={\"window\": 8, \"keep_system_first\": True},  # different config is fine\n",
    ")\n",
    "\n",
    "# Load the saved memory into this session\n",
    "conv_fixed_2.load(\"mem_fixed.json\")\n",
    "\n",
    "# Ask a question to prove memory came back\n",
    "resp = conv_fixed_2.chat(\"What is my favorite color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0oFhxt46mFv"
   },
   "source": [
    "After saving the conversation state, we can inspect the JSON file directly to see what was stored. The command below shows the first few lines of the saved file, giving us a look at how the strategy, messages, and session details are represented.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diSi6Nw017DK",
    "outputId": "f807d1e2-3a7f-45bd-fc93-a08fbba97d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": 1,\n",
      "  \"session_id\": \"5c02d3e2-e8d7-4ed4-8f8e-f864f5763522\",\n",
      "  \"strategy_state\": {\n",
      "    \"strategy_name\": \"fixed\",\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"type\": \"human\",\n",
      "        \"data\": {\n",
      "          \"content\": \"Hi, my name is Jeff.\",\n"
     ]
    }
   ],
   "source": [
    "!head mem_fixed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vGbf5pB0jzk"
   },
   "source": [
    "### Example: Summarizing memory\n",
    "\n",
    "This next example shows how the summarizing strategy works in practice. Instead of keeping every message in memory, older turns are compressed into a running summary while the most recent exchanges remain verbatim. We begin by starting a session, teaching the model a fact, and adding enough small talk to trigger summarization. After saving the session, we load it into a new conversation and confirm that the important detail still survives through the summary mechanism.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m-i6sP0c0ncL",
    "outputId": "257f2d3e-4ff7-4265-fab6-caf1bf7f97fe"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello assistant."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember my dog's name is Hickory."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Got it — your dog's name is Hickory. I'll remember that for the rest of this conversation. If you want me to remember it across future sessions, I don't automatically retain details between chats unless the platform provides a persistent memory feature — would you like me to save it if that's available?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #0: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #1: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #2: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #3: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #4: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #5: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #6: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #7: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #8: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #9: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #10: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #11: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #12: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #13: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #14: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Small talk line #15: noting something irrelevant. Just say OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Your dog's name is Hickory.\n"
     ]
    }
   ],
   "source": [
    "# --- Summarizing strategy example ---\n",
    "# Assumes the ChatConversation and strategies code from before is already defined in the notebook.\n",
    "\n",
    "# 1) Start a session with summary strategy, teach a fact\n",
    "conv_sum_1 = util_chat.ChatConversation(\n",
    "    strategy_name=\"summary\",\n",
    "    strategy_kwargs={\n",
    "        \"keep_last\": 8,\n",
    "        \"trigger_len\": 12,\n",
    "        \"max_summary_chars\": 800,\n",
    "        \"system_prefix\": \"Running summary\",\n",
    "    },\n",
    ")\n",
    "conv_sum_1.chat(\"Hello assistant.\")\n",
    "conv_sum_1.chat(\"Please remember my dog's name is Hickory.\")\n",
    "\n",
    "# Add enough chatter to trigger summarization in normal use\n",
    "for i in range(16):\n",
    "    conv_sum_1.chat(f\"Small talk line #{i}: noting something irrelevant. Just say OK if you got it.\")\n",
    "\n",
    "# Save after some turns (summary strategy persists both the raw tail and the compact summary)\n",
    "conv_sum_1.save(\"mem_summary.json\")\n",
    "\n",
    "# 2) New session (could be a new process), load memory, and verify it works\n",
    "conv_sum_2 = util_chat.ChatConversation(\n",
    "    strategy_name=\"summary\",\n",
    "    strategy_kwargs={\"keep_last\": 6, \"trigger_len\": 10},  # different config is fine\n",
    ")\n",
    "\n",
    "# Load the saved memory into this session\n",
    "conv_sum_2.load(\"mem_summary.json\")\n",
    "\n",
    "# Ask a question to prove the fact survived via the running summary\n",
    "resp = conv_sum_2.invoke(\"What is my dog's name?\")\n",
    "print(\"AI:\", resp.content)  # Expect: \"Hickory\" (or equivalent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NZSM43x652C"
   },
   "source": [
    "Just like before, we can take a look at the saved JSON file to see how the summarizing strategy records its state. The command below prints the first few lines, which will include both the compact summary and the recent tail of the conversation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09K73-E01xV0",
    "outputId": "604732e0-e67e-4a01-b54f-475ac2c3cca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": 1,\n",
      "  \"session_id\": \"8ae9df68-cbda-4a94-9b62-6c8853d1c270\",\n",
      "  \"strategy_state\": {\n",
      "    \"strategy_name\": \"summary\",\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"type\": \"human\",\n",
      "        \"data\": {\n",
      "          \"content\": \"Hello assistant.\",\n"
     ]
    }
   ],
   "source": [
    "!head mem_summary.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd7UuRNR5MtJ"
   },
   "source": [
    "### Undo and Regenerate\n",
    "\n",
    "The final example demonstrates the default passthrough strategy, which keeps the entire conversation history without trimming or summarizing. In this case, we also explore the editing controls that `ChatConversation` provides. We start by introducing some context, then use `undo()` to remove the most recent interaction. Afterward, we continue the conversation and finally call `regenerate()` to replace the last AI response with a fresh one. This shows how you can manage and refine conversations interactively while retaining full history.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "HdYIAFJo5PiJ",
    "outputId": "461c7137-664d-4715-da54-fe2db3477416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello, my name is Jeff."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hi Jeff, nice to meet you—how can I help today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I like coffee."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Nice—what's your favorite kind of coffee or how do you like it prepared?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What is my name."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do I like coffee?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I don't know whether you like coffee, Jeff—do you?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human (regenerated):** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do I like coffee?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI (regenerated):** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I don't know whether you like coffee—do you?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the conversation with default passthrough strategy\n",
    "c = util_chat.ChatConversation(system_prompt=\"You are a helpful assistant that answers in single sentences.\")\n",
    "print(\"---\")\n",
    "c.chat(\"Hello, my name is Jeff.\")\n",
    "print(\"---\")\n",
    "c.chat(\"I like coffee.\")\n",
    "print(\"---\")\n",
    "c.undo()\n",
    "print(\"---\")\n",
    "c.chat(\"What is my name.\")\n",
    "print(\"---\")\n",
    "c.chat(\"Do I like coffee?\")\n",
    "c.regenerate()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
