{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_06_4_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 6: Retrieval-Augmented Generation (RAG)**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 6 Material\n",
    "\n",
    "* Part 6.1: Introduction to Retrieval-Augmented Generation (RAG) [[Video]](https://www.youtube.com/watch?v=qA52K0K181Q) [[Notebook]](t81_559_class_06_1_rag.ipydb)\n",
    "* Part 6.2: Introduction to ChromaDB [[Video]](https://www.youtube.com/watch?v=R53lo4sevLQ) [[Notebook]](t81_559_class_06_2_chromadb.ipynb)\n",
    "* Part 6.3: Understanding Embeddings [[Video]](https://www.youtube.com/watch?v=Tq82Gl2ZZNM) [[Notebook]](t81_559_class_06_3_embeddings.ipynb)\n",
    "* **Part 6.4: Question Answering Over Documents** [[Video]](https://www.youtube.com/watch?v=hCwL_lW-gP0) [[Notebook]](t81_559_class_06_4_qa.ipynb)\n",
    "* Part 6.5: Embedding Databases [[Video]](https://www.youtube.com/watch?v=BG2gT4uYxhM) [[Notebook]](t81_559_class_06_5_embed_db.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "31f09d94-4c79-4569-c40a-0f0ba9f0c36c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai chromadb langchain_community sentence-transformers langchainhub pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 6.4: Question Answering Over Text Documents\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an advanced technique designed to enhance the capabilities of large language models (LLMs) by integrating external data into the response generation process. For RAG to be truly effective, it must operate over data that is not already present in the foundation model. This is crucial because the primary advantage of RAG lies in its ability to pull in specific, often up-to-date information that the LLM might not have been exposed to during its initial training.\n",
    "\n",
    "This feature makes RAG particularly valuable for corporate environments. Companies generate and store vast amounts of proprietary data, including internal documents, customer information, and detailed reports. This data is often unique to the organization and not part of the publicly available corpus that an LLM would be trained on. By using RAG, businesses can leverage their own data to obtain more precise and contextually relevant answers from their LLMs, thereby improving decision-making processes and operational efficiency.\n",
    "\n",
    "To illustrate the capabilities of RAG, we will utilize a sample dataset I created, containing synthetic data of employee biographies from five fictional companies. This dataset is crafted to demonstrate how RAG can effectively retrieve and utilize specific information that a foundation model would not inherently possess. By doing so, it provides a clear example of how RAG can be applied in a real-world corporate setting.\n",
    "\n",
    "Here is an example of such a generated biography:\n",
    "\n",
    "> Elena Martinez is a seasoned Robotics Engineer at FutureTech, a leading innovator in artificial intelligence and robotics based in Silicon Valley. With a Master's degree in Mechanical Engineering from MIT and over a decade of experience, Elena has been pivotal in the development of autonomous robotic systems designed to enhance urban mobility and accessibility. Her groundbreaking work includes the creation of the first AI-powered robotic assistant that can seamlessly interact with urban environments to aid the elderly and disabled. A passionate advocate for women in STEM, Elena also leads FutureTech's outreach program, aiming to inspire the next generation of female engineers through workshops and mentorships. Her contributions have not only propelled FutureTech to new heights but have also set new standards in robotics applications for social good.\n",
    "\n",
    "\n",
    "This biography showcases the type of detailed, company-specific information that RAG can retrieve and incorporate into its responses, enhancing the relevance and accuracy of the generated content. Through the integration of such tailored data, RAG not only enriches the output of LLMs but also ensures that the responses are aligned with the unique context and needs of the organization.\n",
    "\n",
    "This sample data is stored at the following URL's, each file holds people from one of the companies.\n",
    "\n",
    "* https://data.heatonresearch.com/data/t81-559/bios/DD.txt\n",
    "* https://data.heatonresearch.com/data/t81-559/bios/FT.txt\n",
    "* https://data.heatonresearch.com/data/t81-559/bios/GS.txt\n",
    "* https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\n",
    "* https://data.heatonresearch.com/data/t81-559/bios/TI.txt\n",
    "\n",
    "The following code defines these URL's and instanciates an LLM model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QQm0vSmeF5T"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import display_markdown\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.inmemory import InMemoryVectorStore\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.2,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "urls = [\n",
    "    \"https://data.heatonresearch.com/data/t81-559/bios/DD.txt\",\n",
    "    \"https://data.heatonresearch.com/data/t81-559/bios/FT.txt\",\n",
    "    \"https://data.heatonresearch.com/data/t81-559/bios/GS.txt\",\n",
    "    \"https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\",\n",
    "    \"https://data.heatonresearch.com/data/t81-559/bios/TI.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmQJoW7Si3Lc"
   },
   "source": [
    "\n",
    "The function processes a large document by dividing it into smaller, manageable segments, known as chunks, to create embeddings for LLM retrieval in Retrieval-Augmented Generation (RAG). The chunk parameter determines the maximum length of each segment, ensuring the text is broken down into parts that can be efficiently handled and analyzed by the language model. The overlap parameter specifies the number of tokens that should be repeated at the beginning of each new chunk, creating an overlap between consecutive chunks. This overlap helps maintain contextual continuity across chunks, improving the quality and accuracy of the embeddings. By systematically segmenting the document and generating embeddings for each chunk, the function enhances the language model's ability to retrieve relevant information, leading to more precise and contextually appropriate responses in the RAG framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNwkulAagbQr"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yojogrn2jQjQ"
   },
   "source": [
    "In this function, we set the parameters for chunk size and overlap to the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnPD5l7oghJR"
   },
   "outputs": [],
   "source": [
    "chunk_size = 900\n",
    "overlap = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBBhD2Utjn8t"
   },
   "source": [
    "We are processing text files that contain lists of people's biographies. These settings help manage the large text data more effectively. The chunk size of 1000 ensures that each segment of text, or chunk, contains up to 1000 tokens, making it easier for the language model to handle and generate embeddings for each part. The overlap of 200 tokens means that each new chunk will start with the last 200 tokens from the previous chunk. This overlap is crucial in maintaining the continuity of the context between chunks, which is particularly useful when dealing with biographical data where details often span across multiple chunks.\n",
    "\n",
    "\n",
    "The following code reads text content from a list of URLs, processes this content by splitting it into smaller, manageable chunks, and stores these chunks as Document objects. Initially, an empty list named documents is created to hold these Document objects. The code then iterates over each URL in the provided list, printing a message to indicate the current URL being processed. For each URL, it fetches the content using the requests.get method and checks for any HTTP errors with response.raise_for_status(). Once the content is successfully retrieved, it is split into smaller segments using the chunk_text function, which takes into account the specified chunk size and overlap to maintain contextual continuity between chunks. These chunks are then used to create Document objects, each containing a portion of the text. These Document objects are subsequently appended to the documents list, effectively organizing the large text files into smaller, retrievable segments suitable for further processing, such as generating embeddings for LLM retrieval in Retrieval-Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zoC7v0igkkk",
    "outputId": "9a389c7f-1d52-4642-83f7-f24d42a72c9f"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Reading: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    content = response.text\n",
    "    chunks = chunk_text(content, chunk_size, overlap)\n",
    "    for chunk in chunks:\n",
    "        document = Document(page_content=chunk)\n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM0kGSyMnjEP"
   },
   "source": [
    "We've loaded the biography data and are now integrating it into the ChromaDB database using the following code. ChromaDB, requiring an embeddings model, defaults to \"all-MiniLM-L6-v2\". Other [model wrappers](https://docs.trychroma.com/integrations/openai) are provided. This model is provided by SentenceTransformerEmbeddings, which is instantiated here for embedding functions. Utilizing CharacterTextSplitter from langchain_text_splitters, documents are chunked into manageable parts for processing. Chroma, a vector store, interfaces seamlessly with these components, enabling efficient storage and retrieval of embeddings within the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460,
     "referenced_widgets": [
      "68d3bb013df845cbb766debf4092a42f",
      "df54569c71fa4bf1bc9800f5b006e4ce",
      "f1b570993843424588d4c9886cd7f8d3",
      "d3f8d53df7834960bf37dab640ff80eb",
      "3d24a4035b2f4c17ac9d94df3139cbc0",
      "3464bfc97663437d908a27407d9c1817",
      "4f69a1b2e20a4d43be5e78ed31256c5f",
      "7f15910219b4424e8c0aa239271abe60",
      "cd726be433b14a92bfd43b0e63fae456",
      "ec3634f8d68a4ef38601250868373a43",
      "3fa6d2df1c8a4a1994b779cecfa48724",
      "f930c3724d55417987e6405c62fc77f6",
      "6190be32427343638cb01f214f6aad59",
      "c483d715a70e4fc7b6cc3fe4c9161f1d",
      "2b6b3e8e206f49dc9a3de5e0fae0fc66",
      "74fc312878524522afb8b12f153880c3",
      "4203e7fe06cc4e69a95c001e4c0c1695",
      "676b4497687649e3b241c42fffb39d5f",
      "34faf5343a89415ca6050494d495db8e",
      "09d9d0d6f3e5466cb92a777e968b57c4",
      "6218403192ab4045ab0f88a18528d853",
      "2778911f7fbb4978b7e82e7a5d42a133",
      "3ebe3186f7504c478fc78c37d8840630",
      "2632e36ffa7b42a69f68e5c3e3113996",
      "f7b1a79b86f4406eb888233b2f781b97",
      "f39d48d775f448658fb18a26a11e03b0",
      "9cfdeda4169f4e3b96d396bb774069ff",
      "c47aaa18871e46229bb238ca95342050",
      "54239e23ecab43bebc3c2ec357a64214",
      "da9d15e316f14e319ffdd052832c4f6d",
      "cf3b7e99534e4d469d59e03eb0eb41ee",
      "5c3d30696fb4465fbd1df0af8a7310f9",
      "8574170534794badb9d7d8395a3e2933",
      "0a9d1a7adb5e42898c28082f3e9cf32e",
      "618838010882444c9e5cbbe5f8c72afc",
      "6a3ab197863e42c5881b0ac129caca50",
      "5e73526146bb47efa2b308dc2d35008a",
      "519bb649f06e4719982ce7588f9c4b2b",
      "1ab76287868749b8a717a8c1626c925e",
      "e71f543d4739449da03c65fb3026ddee",
      "24ef7cde093645feb32f8acb92265bb5",
      "28a88f02acbb4b38b474c219070317ad",
      "2c27781d15ca4a6e9d775af2952e701d",
      "62933e98a3844a75aa9cc8ad160adc68",
      "a574a476accc483eb9c70c86cfc03204",
      "366967a33f584ce1a1f2d9ca1ee3e53f",
      "e8227ffeefa0472b99a602d15d06be09",
      "73011d7a22474092bc896c16ba56b99f",
      "0d9b67a692de4612a8fb46f65118c87c",
      "b5df88cc12ed43ef81ff5dc4e668513b",
      "76b5503fb9a94ecca076dc0cc8a5776a",
      "eb9729256b2b47efa5cd8393f696e5c3",
      "b25663cd25994c7c9e732878b58749ca",
      "6b959d4da8144fcdb63249e7f14e67bd",
      "6c53836a08a241d2a4ba18fcda3fa57f",
      "c5a518da4d444455b92b4c6c0ef25e32",
      "4b20a23a49884155869d49a1ac1fdf76",
      "cbecc3c56d914069aa870a51c974ac7a",
      "feef3088b25a47c28c730b96b537bcfb",
      "a755624dfd1147e4b96eb6d636760250",
      "f3df4a3a0e0447fd8732e2c9b6abe78d",
      "a7dde37db3bf444480ae24589347b3c6",
      "9c53fe5575074b64b1d56156898193e1",
      "0874eba30374497ea28b1e8430337e4b",
      "03e81b34733b4a6989e80af808ad6dfb",
      "1fa41bc1c41046aaa32a7f02924dd144",
      "eaf503e2a7f5411cac863c93c059c9f9",
      "163bd834a8d142deaac8437f9e45f233",
      "6b980c6821ce4712ab265d4782261621",
      "66bdae3d974f43b4804dab6bddbf341f",
      "ce89717b370845e3ab6e7a384678e0fa",
      "0a407c86444d462da26efcfb2d1f9814",
      "839d57aec9e9461e840f28fe63320c90",
      "6f5f821a5c4949278167b3eb5d8fe808",
      "8a612ea80a554531946fe624e1deff71",
      "d38d0ba254cb417c90338a27ec5153b4",
      "2bd01c618ab346c79be6620a3e4d6e37",
      "06811b0df5834c62a409c043ace5f0bf",
      "5269f9d43cec468abc71d950a2972a6b",
      "206c2733ac4f40ea81b97b1457f92671",
      "a06cdd9cb67a4361a432d4ad4b2b5e39",
      "bde2b14926014c99b205420a68f98ca0",
      "84b6b7907435475887fc51822f3d7bd0",
      "6bf186ca30df4ae4a49fd319291de0c5",
      "a07b7849eb6d4a06ac63324a4f20dd65",
      "c12033a292e244038b60cdd8593281aa",
      "31a69d7afc7b4c889ea84c52e1389107",
      "c4f5f466bcf34d5e8a1fd9f93a34d551",
      "9b5299a5c0dc4c51a997fd043782e3f1",
      "ea14c88c79174f04b00cbca4c04b7817",
      "17d7bcf0ef5e4f2c9b51e243c3d48dc6",
      "ef4a1142789e47caa973209b5e1536d4",
      "ac756588e4f541f78fb8ad7a199741d8",
      "72b31e123a5449f7b0711d615d8cf6b8",
      "7cb3a3aedd96469b835bb08eaf096c01",
      "5b025d6a8f0e410db574ad421e3bf79b",
      "cecf1f88044f4e859e5393699c791b38",
      "7ec3c79ae64e41f1836dd65735ff6dcf",
      "c690ad90184744bfac757b113814992d",
      "54dac098381d48a7a73bc2d439064b7a",
      "6de619a422a949b09462ee5aa5d22913",
      "1e7234a3193b43f1b6cdcc4c7890e267",
      "1223a20abfe745a7b6109b2d43890bfb",
      "351ece73cd194d1ca76a659a6ed035eb",
      "e19be6872e7f4b62bb5fd206bd8d8764",
      "714ca8e211364d5c83de580c62d80438",
      "4c0f09e5e5a1449cbbcaba03aafafb16",
      "3c3a26d6ec86499482f3cded50808a3f",
      "e7af2fc7c6e348119e84b83aea5fd5b5",
      "4573220908094ad2ab3dd430b9c646d3",
      "d3cc91ee69f34653ab8105b9954d5e9d",
      "8fd18c9b7d974d23b7ff6d3676bb58e9",
      "4838a3993a8546da92cad577274ace7d",
      "88370c77fe9746a4b77c369c0ef7cd90",
      "e989bd288482442ebaf63f5baa4919e9",
      "7034eb8eb44f43afa9972290f1c615c9",
      "e43c1c477f214d06916eeebbdb926b05",
      "1186d4d94e4b4c099de5a0d36e44530a",
      "fbbc7ca4072d4f9bad237443e3832a52",
      "27dc08a3c2f34101a0839a016071f3ad",
      "c7e89fea1c8a465bb4734216082261b9"
     ]
    },
    "id": "wyBkIzgdPAd-",
    "outputId": "eb2df963-13b0-4150-81c9-8d2a8555718b"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7RHY_3h6qQ8"
   },
   "source": [
    "We're set to construct a RAG (Retrieval-Augmented Generation) chain with the following code. Starting with a retriever initialized from our previously configured ChromaDB database, documents are formatted into a concatenated string for input preparation. The chain incorporates a question-answer flow: the formatted documents serve as context, alongside a pass-through question handler. Utilizing the RAG model retrieved from the hub, the chain proceeds to a language model (llm). Finally, results are parsed into a string format using StrOutputParser, completing the process for generating answers based on retrieved document contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2BSjxvDPrND",
    "outputId": "68261c69-fa59-49dd-8092-1a361a686ee3"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_documents(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIjLhtCr7HLe"
   },
   "source": [
    "We can now invoke the RAG chain and query it about one of the people who were inside of our sampe biography data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "139UgQ5H34sL",
    "outputId": "ae432e2f-076c-434f-f44f-2868646a7132"
   },
   "outputs": [],
   "source": [
    "qa_chain.invoke(\"What company does Elena Martinez work for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGb8_A28nH4L"
   },
   "source": [
    "## Taking Apart the RAG Chain\n",
    "\n",
    "Several components combine to bring external data into a prompt for RAG-style access. In this section, we will examine each and see how it functions individually. We begin by seeing how to prompt ChromaDB and watch it retrieve relevant information from the larger set of documents. This smaller subset allows the data to fit into the context. Even if the entirety of the source material would fit into the context buffer of the LLM, RAG would decrease costs and improve access time by requiring much less data to be sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMAOksLu30kS",
    "outputId": "517d9f99-d8ec-4dbe-b10e-c43ff0627159"
   },
   "outputs": [],
   "source": [
    "# query it\n",
    "query = \"What company does Elena Martinez work for?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARbwrvd5_Xwf"
   },
   "source": [
    "As you can see, ChromaDB returns multiple bits of information that will give the LLM context to answer the question. None of this information is \"common knowldge,\" it only exists in the generated biography data we loaded into ChromaDB.\n",
    "\n",
    "We will also look at the RAG prompt template. We use the standard prompt template provided by LangChain Hub. The text of this prompt is here. Both the question and any supporting information from the database are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "OYJD0hgE_bqF",
    "outputId": "164ec10f-e71f-4239-ad0c-e3e17a20dcf9"
   },
   "outputs": [],
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_prompt.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct13EpDFArFr"
   },
   "source": [
    "The R in RAG standards stands for \"retrieval.\" Let's pass the question to the retriever object, which will query ChromaDB to find which documents most closely match. You will see some overlap and duplication. This overlap helps to ensure continuity across chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrOXOMe1Axix",
    "outputId": "97755884-188f-4694-b869-bae536e39d1d"
   },
   "outputs": [],
   "source": [
    "retriever.invoke(\"What company does Elena Martinez work for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JFStTE3CMsE"
   },
   "source": [
    "This data is combined with the question into the RAG prompt to be submitted to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARR5NeLzQ2Wg"
   },
   "source": [
    "## RAG Over PDF Documents\n",
    "\n",
    "We will now examine how to use RAG with a PDF document. We will discuss a PDF I generated with the book creator we saw earlier in this class. The book is in the [steampunk](https://en.wikipedia.org/wiki/Steampunk) genre and is titled [Clockwork Dreams and Brass Shadows](https://data.heatonresearch.com/data/t81-559/assignments/clockwork.pdf). We use the same code as before, except I convert the PDF to text before generating chunks. This technqiue will be very helpful to you for Assignment 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2vGastaRds8",
    "outputId": "09c29c69-a3e5-48ec-8449-df26759ca7d9"
   },
   "outputs": [],
   "source": [
    "import pypdf\n",
    "from io import BytesIO\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.2,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "urls = [\n",
    "    \"https://data.heatonresearch.com/data/t81-559/assignments/clockwork.pdf\"\n",
    "]\n",
    "\n",
    "def extract_pdf_text(pdf_content):\n",
    "    pdf_file = BytesIO(pdf_content)\n",
    "    reader = pypdf.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "chunk_size = 900\n",
    "overlap = 300\n",
    "\n",
    "documents = []\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Reading: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    content = extract_pdf_text(response.content)  # Corrected to extract text using pypdf\n",
    "    chunks = chunk_text(content, chunk_size, overlap)\n",
    "    for chunk in chunks:\n",
    "        document = Document(page_content=chunk)\n",
    "        documents.append(document)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_documents(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_WXhc5MU-gU"
   },
   "source": [
    "Now that we have loaded the PDF, we can query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "OvwT9uj6R3Nv",
    "outputId": "210059e6-3877-45fe-aeab-71f54ca54948"
   },
   "outputs": [],
   "source": [
    "qa_chain.invoke(\"Who is Eliza Hawthorne?\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
