{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whjsJasuhstV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_01_4_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOZxlIMhstX"
      },
      "source": [
        "# T81-559: Applications of Generative Artificial Intelligence\n",
        "**Module 1: Course Overview**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Yov72PhstY"
      },
      "source": [
        "# Module 1 Material\n",
        "\n",
        "* Part 1.1: Course Overview [[Video]](https://www.youtube.com/watch?v=OVS-6s20Ms0) [[Notebook]](t81_559_class_01_1_overview.ipynb)\n",
        "* Part 1.2: Generative AI Overview [[Video]](https://www.youtube.com/watch?v=ohmPaSsKhMs) [[Notebook]](t81_559_class_01_2_genai.ipynb)\n",
        "* Part 1.3: Introduction to OpenAI [[Video]](https://www.youtube.com/watch?v=C2xyi2Cq-bU) [[Notebook]](t81_559_class_01_3_openai.ipynb)\n",
        "* **Part 1.4: Introduction to LangChain** [[Video]](https://www.youtube.com/watch?v=qQI5AhaKxuI) [[Notebook]](t81_559_class_01_4_langchain.ipynb)\n",
        "* Part 1.5: Prompt Engineering [[Video]](https://www.youtube.com/watch?v=_Uot1i5sIXo) [[Notebook]](t81_559_class_01_5_prompt_engineering.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcAUP0c3hstY"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running and maps Google Drive if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsI496h5hstZ",
        "outputId": "0ed67b14-b2b9-4097-bb8c-faa85a0e5c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.30)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.99.9)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# OpenAI Secrets\n",
        "if COLAB:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Install needed libraries in CoLab\n",
        "if COLAB:\n",
        "    !pip install langchain langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC9A-LaYhsta"
      },
      "source": [
        "# Part 1.4: Introduction to LangChain\n",
        "\n",
        "One of the most intriguing and promising developments in the evolving landscape of language models and artificial intelligence is LangChain. This technology represents a significant leap forward in how we interact with and harness the capabilities of large language models (LLMs). As we delve into the intricacies of LangChain in this chapter, it's important to understand not just the technical underpinnings but also the user experience that makes it so revolutionary.\n",
        "\n",
        "## LangChain Chat Conversation Format\n",
        "\n",
        "To explore LangChain comprehensively, we will adopt a format that has become increasingly familiar and effective in LLMs: the chat conversation interface. This interactive style, reminiscent of how many of us communicate daily, offers a unique and accessible means to illustrate LangChain's capabilities, potential applications, and the nuances of its operation.\n",
        "\n",
        "We begin by importing the components from the LangChain library to support a chat-style interface to OpenAI. We will use the ChatOpenAI interface for the OpenAI family of LLM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2lC_JK3guaj"
      },
      "outputs": [],
      "source": [
        "# Conversation Style Inteface\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWHTAub0sTm"
      },
      "source": [
        "The conversation format consists of arrays of chat entries of the following three types:\n",
        "\n",
        "* **SystemMessage** - This class designates the system prompt that provides instructions to the AI on the nature of the conversation and hints and guidelines. Generally, there will be only one system message at the beginning of the array.\n",
        "* **HumanMessage** - This class designates the chat messages from outside the LLM, typically the human user.\n",
        "* **AIMessage** - This class designates the chat messages from the LLM as responses to the HumanMessage messages.\n",
        "\n",
        "Here we see the chain to ask a simple question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_sJoVYAtE6b"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a helpful assistant that concisely and accurately answers questions.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=\"What is the capital of France?\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To7rZtUO4B8t"
      },
      "source": [
        "We now submit these messages and retrieve the output from the model. We will use gpt-4o-mini, which is good enough for this query. Further, we use a zero temperature; we are simply looking for a factual answer, and creativity is not a goal or concern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfb875JhtI6J",
        "outputId": "cb6a3eaf-57d0-41b6-e751-66a361c1cf63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n",
            "The capital of France is Paris.\n",
            "-----------\n",
            "{'token_usage': {'completion_tokens': 16, 'prompt_tokens': 31, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C5bCdujUreNPvy9AUeQY2CHrwrkfS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "MODEL = 'gpt-5-mini'\n",
        "\n",
        "# Initialize the OpenAI LLM with your API key\n",
        "llm = ChatOpenAI(\n",
        "  model=MODEL,\n",
        "  temperature= 0.0,\n",
        "  n= 1,\n",
        "  max_tokens= 256)\n",
        "\n",
        "print(\"Model response:\")\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)\n",
        "print(\"-----------\")\n",
        "print(output.response_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6a0hXj4lHj"
      },
      "source": [
        "The model that LangChain returns to you returns additional metadata. This data shows the token usage, which might be useful for estimating the total cost expected from this query.\n",
        "\n",
        "We can continue to grow this conversation if we wish. To do so, we added the model's response and another human question. Here, we will ask the model if it was sure about its last response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vJuMVHZ1jJ8",
        "outputId": "4b743537-b420-4da6-9b0f-a4a5b5bf66aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SystemMessage : You are a helpful assistant that concisely and accurately answers questions.\n",
            "HumanMessage : What is the capital of France?\n",
            "AIMessage : The capital of France is Paris.\n",
            "HumanMessage : Are you sure, I think it was renamed for some reason?\n"
          ]
        }
      ],
      "source": [
        "messages.append(output)\n",
        "messages.append(HumanMessage(content=\"Are you sure, I think it was renamed for some reason?\"))\n",
        "for message in messages:\n",
        "    print(f\"{type(message).__name__} : {message.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Z7DZRH5TFr"
      },
      "source": [
        "We can submit the conversation array to the model and see its latest response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qFxkEmAz1w1",
        "outputId": "4b9006e9-e119-446f-e348-1496153e8f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Model response:\")\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9nvX9oA3a0Z"
      },
      "source": [
        "## Asking a Single Question\n",
        "\n",
        "If you wish to ask the model a single question, not as part of a conversation chain, you can pass a string to the model for a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "34-95k0qg8ss",
        "outputId": "d90ca85a-9aa5-4a40-964f-38ea76998add"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'By the 2020 U.S. Census, the five largest U.S. cities by population were:\\n1. New York, NY — 8,336,817  \\n2. Los Angeles, CA — 3,898,747  \\n3. Chicago, IL — 2,746,388  \\n4. Houston, TX — 2,304,580  \\n5. Phoenix, AZ — 1,608,139\\n\\nIf you want the most recent population estimates (e.g., 2022–2023 Census Bureau estimates), I can provide those instead.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# complete\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "MODEL = 'gpt-5-mini'\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "llm = ChatOpenAI(model=MODEL, temperature=0)\n",
        "\n",
        "# Define the question\n",
        "question = \"What are the five largest cities in the USA by population?\"\n",
        "\n",
        "# Use Langchain to call the OpenAI API\n",
        "# The method and parameters might differ based on the Langchain version\n",
        "response = llm.invoke(question)\n",
        "\n",
        "# Print the response\n",
        "display(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o2EkUkN3hEX"
      },
      "source": [
        "## Prompt Templates\n",
        "\n",
        "LangChain allows you to create chains of operations typically performed as part of an LLM-enabled application. One of these operations is a prompt template, which allows you to insert text into a previously created prompt. In this example, we will create a prompt template that asks the model to create a random blog post title.\n",
        "\n",
        "```\n",
        "Return only the title of a blog post article title on the topic of {topic} in {language}\n",
        "```\n",
        "\n",
        "To accomplish this objective, we will use a **PromptTemplate** object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-g3zOXBmulc",
        "outputId": "71ce4a8a-b301-41d8-bd37-333b1797c9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How My Cat Helped Me Tune Hyperparameters: Pet-Powered Lessons for Data Scientists\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "topic = \"pets for data scientists\"\n",
        "language = \"english\"\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "# Use higher temperature for greater creativity\n",
        "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
        "\n",
        "# Define the prompt template\n",
        "title_template = PromptTemplate(\n",
        "    input_variables=['topic', 'language'],\n",
        "    template='Return only the title of a blog post article title on the topic of {topic} in {language}'\n",
        ")\n",
        "\n",
        "# Use RunnableSequence for chaining\n",
        "title_chain = title_template | llm #RunnableSequence(steps=[title_template, llm])\n",
        "\n",
        "# Invoke the chain with inputs\n",
        "response = title_chain.invoke({'topic': topic, 'language': language})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOpAcj-byBPg"
      },
      "source": [
        "## Create a Simple Sequential Chain\n",
        "\n",
        "We will now use LangChain to tie multiple LLM calls into a longer chain using the **SimpleSequentialChain** class. We will use two smaller chains to create a title and body text for a blog post. We begin by defining the two prompts we will use to construct this blog post. Also, note that we request that the LLM utilize [markdown](https://en.wikipedia.org/wiki/Markdown) to generate the actual blog post.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELS-9JD3Sao6"
      },
      "outputs": [],
      "source": [
        "# Create the two prompt templates\n",
        "title_template = PromptTemplate( input_variables = ['topic'], template = 'Give me a blog post title on {topic} in English' )\n",
        "article_template = PromptTemplate( input_variables = ['title'], template = 'Write a blog post for {title}, format in markdown.' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3vy7jf3-_xl"
      },
      "source": [
        "We will create the first chain to generate the random title. Here, we allow the user to specify the topic. We use a higher temperature to increase the creativity of the title. We also use a simpler model to minimize cost for the relatively simple task of title selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KV8UJUDyP8M"
      },
      "outputs": [],
      "source": [
        "MODEL = 'gpt-5-mini'\n",
        "\n",
        "# Create a chain to generate a random\n",
        "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
        "title_chain = title_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO0ielMf_xRa"
      },
      "source": [
        "Next, we compose the actual blog post; we will use a lower temperature to decrease creativity and cause the LLM to stick to factual information and avoid hallucinations. We also use a more complex model to provide a better article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7OCQCggyHlB"
      },
      "outputs": [],
      "source": [
        "MODEL2 = 'gpt-5'\n",
        "\n",
        "# Create the article chain\n",
        "llm2 = ChatOpenAI(model=MODEL2, temperature=0.1)\n",
        "article_chain = article_template | llm2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rftFvMyBa4-"
      },
      "source": [
        "Now, we combine these two chains into one. The input to the first chain will be the selected topic. The first chain will then output the title to the second chain, which will, in turn, output the actual article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ibHBt4eyKZt"
      },
      "outputs": [],
      "source": [
        "# Create a complete chain to create a new blog post\n",
        "#complete_chain=SimpleSequentialChain(chains=[title_chain, article_chain], verbose=True)\n",
        "\n",
        "complete_chain = title_chain | article_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_7P-n_DBvpD"
      },
      "source": [
        "We can now display the final article. In this case, we requested an article on \"photography,\" and displayed the final article's markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwro_kfKXvml"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display_markdown\n",
        "\n",
        "article = complete_chain.invoke('photography')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1pN8CbXCE6F"
      },
      "source": [
        "The actual display of the markdown is handled by this code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vt5uGJawuru_",
        "outputId": "cee35b9b-6b03-466f-cefc-f3a6b69db264"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "# Mastering Light: How to Transform Everyday Scenes into Stunning Photographs\n\nLight is the raw material of photography. Learn to see it—and shape it—and even the most ordinary scene becomes compelling. This guide gives you practical ways to read light, adapt fast, and create images with depth, mood, and clarity.\n\n## What Light Really Does\n- Shapes: Side light reveals texture; backlight carves edges; top light flattens.\n- Sets mood: Warm tones feel intimate; cool tones feel calm or stark.\n- Guides attention: Bright areas pull the eye; darker zones recede.\n- Adds depth: Contrast between light and shadow creates three-dimensionality.\n\n## The Four Variables (and how to use them)\n1. Direction\n   - Front: Low contrast, safe but flat. Good for color accuracy.\n   - Side: Adds texture and drama. Great for landscapes, food, portraits.\n   - Back: Glow, rim light, silhouettes. Watch for lens flare—use a hood or hand.\n   - Top/Bottom: Harsh and unflattering. Move your subject or yourself.\n\n2. Quality (hard vs. soft)\n   - Hard: Small, distant sources (noon sun, bare bulb). Crisp shadows, high drama.\n   - Soft: Large, close sources (window, overcast, diffuser). Gentle transitions, flattering skin.\n\n3. Intensity\n   - Control with exposure, ND filters, distance from light, or by moving into shade.\n   - Expose for the highlights; lift shadows later if needed.\n\n4. Color (white balance)\n   - Typical Kelvin cheats: Daylight 5200–5600K, Shade 6500–7500K, Tungsten 2700–3200K, Golden Hour 4000–4800K, Blue Hour 9000–12000K.\n   - Mixed light? Pick the dominant source and correct the rest (gels or post).\n\n## Read the Scene in 30 Seconds\n- Where is the brightest area?\n- Where are the hardest edges in the shadows?\n- What direction is the light coming from relative to your subject’s face/form?\n- Is the color warm/cool? Mixed?\n- What background stays clean at that angle?\n\nMake a choice: move yourself, move the subject, add/subtract light, or change the time of day.\n\n## Make Common Light Work for You\n- Harsh midday sun\n  - Put the sun behind your subject for rim light; expose for the highlights.\n  - Use open shade or create shade with your body/hat.\n  - Add “negative fill” (black side of a reflector, dark jacket) to restore contrast.\n  - Try black-and-white to lean into graphic shapes.\n\n- Overcast days\n  - Soft, forgiving. Create direction by turning your subject toward/away from the brightest patch of sky.\n  - Add a white reflector to open shadows or a black card to add definition.\n  - Watch backgrounds; soft light can look flat without layers.\n\n- Window light\n  - Place the window at 45° for classic modeling. Turn the subject’s nose slightly toward the light (short lighting for slimming).\n  - Control spill with curtains or a flag. Bounce with a white card/foil-lined board.\n  - Set WB to 5000–6000K or use Auto and fine-tune in post.\n\n- Golden hour and blue hour\n  - Golden hour: warm, directional. Shoot side/backlight for glow; try a sunstar at f/16.\n  - Blue hour: cool, even. Balance ambient with practical lights; use a tripod and longer exposures.\n\n- Mixed lighting (office, street, interiors)\n  - Decide on your hero light. Gel your flash to match or move your subject to simplify.\n  - Shoot RAW for maximum flexibility.\n\n## Simple Tools That Change Everything\n- Collapsible reflector (white/silver/black) for fill or negative fill.\n- 5-in-1 diffuser to soften harsh sun.\n- Lens hood or your hand to tame flare.\n- Small flash or LED for a kiss of fill; bounce off walls/ceilings.\n- Polarizer to cut glare and deepen skies (mind the angle to the sun).\n\nDIY: Foam board for bounce, black T-shirt for negative fill, sheer curtain as diffuser.\n\n## Exposure and Color You Can Trust\n- Use your histogram/zebras; protect highlights.\n- Aperture Priority with exposure compensation is fast in changing light.\n- Manual when the light is stable or for consistent series/portraits.\n- Bracket (±2 EV) for high-contrast scenes; blend if needed.\n- Shoot RAW. It’s your insurance for tricky color and dynamic range.\n\n## Composition Meets Light\n- Build around contrast: bright-on-dark or dark-on-bright subject isolation.\n- Angle for separation: a small step can place your subject against a brighter/darker plane.\n- Use leading lines that are lit; a lit line carries the eye.\n- Clean your background; light plus chaos still looks chaotic.\n\n## Mini Field Recipes\n- Noon portrait without gear\n  - Put the sun behind the subject; expose for skin.\n  - Find open shade or use a building edge to block top light.\n  - Add negative fill on the shadow side (dark jacket close to cheek).\n\n- Moody window portrait\n  - Subject one step from window; turn slightly away for short light.\n  - Flag the far side with a dark board for contrast.\n  - Spot meter for the bright cheek; let the background fall to shadow.\n\n- Travel street scene at blue hour\n  - Expose for the sky; let city lights glow.\n  - Add a person crossing the brightest pool of light.\n  - White balance around 8000–10000K to retain deep blues.\n\n- Backlit leaves/food/details\n  - Place light behind and slightly above; shoot toward the light.\n  - Shield the lens from direct rays to avoid flare haze unless you want the glow.\n\n## A One-Week Practice Plan\n- Day 1: Photograph the same object in front, side, and backlight.\n- Day 2: Hard vs. soft—bare bulb vs. diffused window.\n- Day 3: Overcast textures—brick, tree bark, clothing.\n- Day 4: Golden hour portraits—rim and short lighting.\n- Day 5: Blue hour city frames—balance ambient and highlights.\n- Day 6: Mixed light interior—choose and match a hero light.\n- Day 7: Edit day—compare how light changed the story.\n\n## Post-Processing That Honors the Light\n- Start with white balance and exposure. Keep highlights believable.\n- Local dodging/burning to guide the eye; avoid global clarity that crushes midtones.\n- Use color grading to reinforce mood: warm mids for coziness, cooler shadows for calm.\n- Dehaze sparingly; it boosts contrast but can look unnatural in skin.\n\n## Common Mistakes\n- Chasing subjects instead of chasing light.\n- Ignoring backgrounds that fight your light.\n- Overfilling faces until they look flat.\n- Letting mixed color casts go unaddressed.\n\n## A Quick Pre-Shot Checklist\n- Where’s the brightest spot and the darkest?\n- What direction and quality is the light?\n- How will I isolate the subject using light contrast?\n- Is my background helping?\n- Are my highlights safe?\n\n---\n\n## Alternative Title Styles\n- Beginner-friendly: “See the Light: Simple Tricks to Make Any Photo Look Better”\n- Gear-focused: “Light Control 101: Reflectors, Diffusers, and Small Flash for Big Results”\n- Portrait angle: “Shape a Face with Light: Everyday Setups for Flattering Portraits”\n- Travel angle: “Chasing Light on the Road: Turn Ordinary Places into Memorable Photos”\n- Creative angle: “Draw with Shadows: Crafting Mood from Common Light”\n- Mobile angle: “Mastering Light with Your Phone: No Extra Gear Needed”\n\nIf you want, I can turn this into a printable one-page field checklist or tailor a version for portraits, travel, or phone-only shooting."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display_markdown(article.content, raw=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (genai)",
      "language": "python",
      "name": "genai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}