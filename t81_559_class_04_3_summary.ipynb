{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_04_3_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 4: LangChain: Chat and Memory**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 4 Material\n",
    "\n",
    "* Part 4.1: LangChain Conversations [[Video]](https://www.youtube.com/watch?v=-wXT2RlzJec&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_1_langchain_chat.ipynb)\n",
    "* Part 4.2: Conversation Buffer Window Memory [[Video]](https://www.youtube.com/watch?v=G-l3T1Z9CHc&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_2_memory_buffer.ipynb)\n",
    "* **Part 4.3: Chat with Summary and Fixed Window** [[Video]](https://www.youtube.com/watch?v=z0iTmoEgn9U&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_3_summary.ipynb)\n",
    "* Part 4.4: Chat with Persistence, Rollback and Regeneration [[Video]](https://www.youtube.com/watch?v=7QEFjNE6wxs&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_4_persistence.ipynb)\n",
    "* Part 4.5: Automated Coder Application [[Video]](https://www.youtube.com/watch?v=pHcKXOMDZKU&ab_channel=JeffHeaton) [[Notebook]](t81_559_class_04_5_coder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "f67000ca-bff6-4ad9-cd10-90b1be27dfe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.100.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 4.3: LangChain Messaging Strategies\n",
    "\n",
    "This module expands on the earlier `SimpleConversation` wrapper by adding\n",
    "pluggable \"message strategies\" that control how conversation history is stored\n",
    "and presented to the LLM. Previously, every conversation simply kept the full\n",
    "in-memory history and passed it through as context. That works fine for short\n",
    "sessions, but becomes inefficient as chats grow longer.\n",
    "\n",
    "To address this, we introduce three strategy classes:\n",
    "\n",
    "1. PassthroughMessageStrategy\n",
    "   - The original behavior. Keeps all messages verbatim and provides the\n",
    "     complete history to the model each turn.\n",
    "\n",
    "2. FixedWindowMessageStrategy\n",
    "   - Maintains only the most recent portion of the conversation. Older\n",
    "     messages are dropped from the backing store once a configurable\n",
    "     window size is exceeded. This prevents context from growing\n",
    "     indefinitely.\n",
    "\n",
    "3. SummarizingMessageStrategy\n",
    "   - Collapses older messages into a compact, running summary. The summary\n",
    "     is maintained with the help of a lightweight LLM call, while the most\n",
    "     recent messages are kept verbatim. This lets the model retain important\n",
    "     facts and decisions without the cost of replaying the entire chat.\n",
    "\n",
    "The `SimpleConversation` class now accepts one of these strategies (or a\n",
    "factory) when instantiated. This gives you control over how history is\n",
    "managed—whether you want full fidelity, bounded context, or long-term memory\n",
    "via summarization, while keeping the same simple interface for sending and\n",
    "displaying chat turns.\n",
    "\n",
    "### BaseMessageStrategy\n",
    "\n",
    "`BaseMessageStrategy` is the common wrapper class that all message strategies\n",
    "inherit from. Its purpose is to provide a consistent interface for adding,\n",
    "clearing, and retrieving chat messages, while delegating actual storage to an\n",
    "`InMemoryChatMessageHistory` instance. By default it simply passes messages\n",
    "through unchanged, but subclasses override how the `messages` property is\n",
    "exposed to the model—such as trimming to a fixed window or replacing older\n",
    "content with a running summary. This design lets `SimpleConversation` plug in\n",
    "different strategies seamlessly, without having to know the details of how\n",
    "history is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional, List, Dict, Any, Callable, Literal\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "class BaseMessageStrategy:\n",
    "    def __init__(self):\n",
    "        self._backing = InMemoryChatMessageHistory()\n",
    "\n",
    "    # Required by RunnableWithMessageHistory / callbacks\n",
    "    def add_user_message(self, message: str) -> None:\n",
    "        self._backing.add_user_message(message)\n",
    "\n",
    "    def add_ai_message(self, message: str) -> None:\n",
    "        self._backing.add_ai_message(message)\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        self._backing.add_message(message)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        self._backing.add_messages(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self._backing.clear()\n",
    "\n",
    "    @property\n",
    "    def backing_messages(self) -> List[BaseMessage]:\n",
    "        return self._backing.messages\n",
    "\n",
    "    @property\n",
    "    def messages(self) -> List[BaseMessage]:\n",
    "        return list(self._backing.messages)\n",
    "\n",
    "    # Optional: delegate any other unknown attrs to the backing object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._backing, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9kIo4jx8rDC"
   },
   "source": [
    "`PassthroughMessageStrategy` is the simplest strategy implementation. It\n",
    "inherits from `BaseMessageStrategy` and provides the full, unmodified chat\n",
    "history to the model each turn. Nothing is trimmed, summarized, or filtered—\n",
    "the conversation grows indefinitely as more messages are added. This is the\n",
    "baseline strategy and represents the original behavior of\n",
    "`SimpleConversation` before windowing or summarization were introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DL0ppO8bT1gm"
   },
   "outputs": [],
   "source": [
    "class PassthroughMessageStrategy(BaseMessageStrategy):\n",
    "    \"\"\"\n",
    "    No trimming or summarization. Returns the full history.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q21fzRA828R"
   },
   "source": [
    "### FixedWindowMessageStrategy\n",
    "\n",
    "`FixedWindowMessageStrategy` enforces a sliding window over the conversation\n",
    "so that only the most recent portion of the chat is preserved and passed to\n",
    "the model. Older messages beyond this limit are discarded from the backing\n",
    "store. This keeps the prompt size under control while still giving the model\n",
    "recent context.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "window : int\n",
    "    The maximum number of messages (or turns, depending on implementation)\n",
    "    to retain. Once this limit is exceeded, the oldest messages are dropped.\n",
    "keep_system_first : bool, default=True\n",
    "    If True, the very first system message (e.g., your system prompt) is\n",
    "    always preserved, even if the rest of the history is trimmed.\n",
    "\n",
    "```\n",
    "# Keep only the last 12 messages, always pinning the system prompt\n",
    "strategy = FixedWindowMessageStrategy(window=12, keep_system_first=True)\n",
    "conv = SimpleConversation(strategy=strategy)\n",
    "conv.chat(\"Hello!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtHbCNXt9di0"
   },
   "source": [
    "### FixedWindowMessageStrategy\n",
    "\n",
    "`SummarizingMessageStrategy` maintains long-running conversations by\n",
    "collapsing older exchanges into a concise summary while keeping the most\n",
    "recent turns verbatim. This allows the model to retain awareness of key facts\n",
    "and decisions without the cost of replaying the entire history.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "summarizer_llm : ChatOpenAI, optional\n",
    "    The LLM used to generate summaries. If not provided, defaults to a\n",
    "    lightweight GPT-5 mini model with low temperature.\n",
    "trigger_len : int, default=20\n",
    "    The minimum number of unsummarized messages required before a summary\n",
    "    pass is triggered.\n",
    "keep_last : int, default=10\n",
    "    The number of most recent messages to always preserve verbatim in\n",
    "    addition to the running summary.\n",
    "max_summary_chars : int, default=1200\n",
    "    A soft limit on the length of the summary text.\n",
    "system_prefix : str, default=\"Running summary\"\n",
    "    A label prepended to the summary when it is injected as a system\n",
    "    message.\n",
    "\n",
    "**Example**\n",
    "\n",
    "```\n",
    "# Collapse older content but keep the last 10 messages\n",
    "strategy = SummarizingMessageStrategy(keep_last=10, trigger_len=20)\n",
    "conv = SimpleConversation(strategy=strategy)\n",
    "conv.chat(\"Let's start a long discussion...\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kLSG2EgWTtBH"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "class FixedWindowMessageStrategy(BaseMessageStrategy):\n",
    "    \"\"\"\n",
    "    Enforces a hard cap of `window` messages in the backing store.\n",
    "    Optionally keeps the very first System message pinned.\n",
    "    \"\"\"\n",
    "    def __init__(self, window: int = 12, keep_system_first: bool = True):\n",
    "        super().__init__()\n",
    "        assert window > 0\n",
    "        self.window = window\n",
    "        self.keep_system_first = keep_system_first\n",
    "\n",
    "    # ---- write-path enforcement ----\n",
    "    def _trim_in_place(self) -> None:\n",
    "        msgs = self._backing.messages\n",
    "        if not msgs:\n",
    "            return\n",
    "\n",
    "        first_sys = msgs[0] if self.keep_system_first and isinstance(msgs[0], SystemMessage) else None\n",
    "        body = msgs[1:] if first_sys else msgs\n",
    "\n",
    "        # keep only last `window` of the body\n",
    "        if len(body) > self.window:\n",
    "            body[:] = body[-self.window:]\n",
    "\n",
    "        # write back\n",
    "        if first_sys:\n",
    "            msgs[:] = [first_sys] + body\n",
    "        else:\n",
    "            msgs[:] = body\n",
    "\n",
    "    def add_user_message(self, message: str) -> None:\n",
    "        super().add_user_message(message)\n",
    "        self._trim_in_place()\n",
    "\n",
    "    def add_ai_message(self, message: str) -> None:\n",
    "        super().add_ai_message(message)\n",
    "        self._trim_in_place()\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        super().add_message(message)\n",
    "        self._trim_in_place()\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        super().add_messages(messages)\n",
    "        self._trim_in_place()\n",
    "\n",
    "    # read-path (still returns whatever is in backing)\n",
    "    @property\n",
    "    def messages(self) -> List[BaseMessage]:\n",
    "        return self._backing.messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SidO09We-QEc"
   },
   "source": [
    "### SummarizingMessageStrategy\n",
    "\n",
    "`SummarizingMessageStrategy` is designed for long conversations where keeping\n",
    "every individual message would quickly exceed the model’s context window. It\n",
    "retains the most recent `keep_last` messages verbatim, while older messages\n",
    "are periodically collapsed into a compact running summary once the number of\n",
    "unsummarized messages passes `trigger_len`. The summary itself is generated by\n",
    "a lightweight LLM (`summarizer_llm`) and injected back into the conversation\n",
    "as a system message so that the model still has access to important facts,\n",
    "decisions, and constraints. Parameters like `max_summary_chars` and\n",
    "`system_prefix` allow you to control the size and labeling of the summary.\n",
    "This strategy is useful when you want the model to remember key details from\n",
    "earlier parts of a discussion without carrying the full message history\n",
    "forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WmKSL6axTnyf"
   },
   "outputs": [],
   "source": [
    "class SummarizingMessageStrategy(BaseMessageStrategy):\n",
    "    \"\"\"\n",
    "    Maintains a running summary of older turns and keeps the recent tail verbatim.\n",
    "    When `trigger_len` is exceeded, everything except the last `keep_last` messages\n",
    "    is summarized via a compact LLM call to `summarizer_llm`.\n",
    "\n",
    "    The model receives:\n",
    "      [SystemMessage(\"Running summary: ...\"), <last keep_last messages...>]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        summarizer_llm: Optional[ChatOpenAI] = None,\n",
    "        trigger_len: int = 20,\n",
    "        keep_last: int = 10,\n",
    "        max_summary_chars: int = 1200,\n",
    "        system_prefix: str = \"Running summary\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert trigger_len > 0 and keep_last > 0, \"trigger_len and keep_last must be positive\"\n",
    "        self.summarizer_llm = summarizer_llm or ChatOpenAI(model=\"gpt-5-mini\", temperature=0.0)\n",
    "        self.trigger_len = trigger_len\n",
    "        self.keep_last = keep_last\n",
    "        self.max_summary_chars = max_summary_chars\n",
    "        self.system_prefix = system_prefix\n",
    "        self._summary_text: Optional[str] = None\n",
    "        self._last_summarized_idx: int = 0  # index in backing messages up to which we summarized\n",
    "\n",
    "    def _need_summarize(self) -> bool:\n",
    "        return len(self.backing_messages) - self._last_summarized_idx > max(self.keep_last, self.trigger_len)\n",
    "\n",
    "    def _build_summarization_input(self) -> List[BaseMessage]:\n",
    "        msgs = self.backing_messages\n",
    "        head = msgs[: max(0, len(msgs) - self.keep_last)]\n",
    "        # If we already have a summary, include it as context\n",
    "        prompt_msgs: List[BaseMessage] = []\n",
    "        if self._summary_text:\n",
    "            prompt_msgs.append(SystemMessage(content=f\"{self.system_prefix} so far:\\n{self._summary_text}\"))\n",
    "        # Add the head to be collapsed\n",
    "        prompt_msgs.extend(head[self._last_summarized_idx :])\n",
    "        return prompt_msgs\n",
    "\n",
    "    def _summarize_now(self) -> None:\n",
    "        # Compose a tight prompt for the summarizer model\n",
    "        to_collapse = self._build_summarization_input()\n",
    "        if not to_collapse:\n",
    "            return\n",
    "        # Summarizer prompt\n",
    "        prompt = [\n",
    "            SystemMessage(content=(\n",
    "                \"You are a terse assistant that compresses chat histories into factual, \"\n",
    "                \"actionable bullet summaries without losing decisions or instructions.\"\n",
    "            )),\n",
    "            HumanMessage(content=(\n",
    "                \"Summarize the following conversation context into 5–10 compact bullets. \"\n",
    "                \"Keep entities, tasks, decisions, constraints, and important examples. \"\n",
    "                f\"Limit to ~{self.max_summary_chars} characters.\\n\\n\"\n",
    "                \"=== BEGIN CONTEXT ===\"\n",
    "            ))\n",
    "        ]\n",
    "        prompt.extend(to_collapse)\n",
    "        prompt.append(HumanMessage(content=\"=== END CONTEXT ===\\n\\nReturn only the bullets.\"))\n",
    "\n",
    "        summary = self.summarizer_llm.invoke(prompt)\n",
    "        new_summary = summary.content.strip()\n",
    "\n",
    "        # Merge summaries if we already had one\n",
    "        if self._summary_text:\n",
    "            # Briefly re-summarize the combined text to keep it tight\n",
    "            merge_prompt = [\n",
    "                SystemMessage(content=\"Condense two bullet lists into one concise list without redundancy.\"),\n",
    "                HumanMessage(content=f\"List A:\\n{self._summary_text}\\n\\nList B:\\n{new_summary}\\n\\nReturn a single concise list.\")\n",
    "            ]\n",
    "            merged = self.summarizer_llm.invoke(merge_prompt)\n",
    "            self._summary_text = merged.content.strip()\n",
    "        else:\n",
    "            self._summary_text = new_summary\n",
    "\n",
    "        # We have summarized all messages up to the current head\n",
    "        self._last_summarized_idx = max(0, len(self.backing_messages) - self.keep_last)\n",
    "\n",
    "    @property\n",
    "    def messages(self) -> List[BaseMessage]:\n",
    "        # Summarize if needed\n",
    "        if self._need_summarize():\n",
    "            self._summarize_now()\n",
    "\n",
    "        tail = self.backing_messages[-self.keep_last:] if self.keep_last > 0 else []\n",
    "        if self._summary_text:\n",
    "            return [SystemMessage(content=f\"{self.system_prefix}:\\n{self._summary_text}\")] + list(tail)\n",
    "        return list(tail)\n",
    "\n",
    "    def get_summary_text(self) -> Optional[str]:\n",
    "        \"\"\"Return the current running summary, summarizing first if needed.\"\"\"\n",
    "        if self._need_summarize():\n",
    "            self._summarize_now()\n",
    "        return self._summary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYLTYPNR-lAD"
   },
   "source": [
    "`SimpleConversation` is the main wrapper class for running LangChain-based\n",
    "chat sessions. In the original version, it always used a plain in-memory\n",
    "message history with no special handling, so every message was kept and\n",
    "passed through to the model. In this enhanced version, the class supports\n",
    "pluggable *message strategies* that define how the conversation history is\n",
    "managed. You can now choose between three built-in strategies:\n",
    "\n",
    "- `PassthroughMessageStrategy`: the original behavior, keeps the entire\n",
    "  history intact with no trimming or summarization.\n",
    "- `FixedWindowMessageStrategy`: enforces a sliding window so only the most\n",
    "  recent N messages (optionally pinning the first system prompt) are retained.\n",
    "- `SummarizingMessageStrategy`: collapses older messages into a compact,\n",
    "  running summary while preserving a configurable number of the most recent\n",
    "  turns verbatim.\n",
    "\n",
    "This enhancement lets you control how the conversation grows: either unlimited,\n",
    "bounded to a fixed window, or long-term via summarization. The external\n",
    "interface of `SimpleConversation` remains the same (`chat`, `invoke`,\n",
    "`print_history`, etc.), so you can swap strategies without changing any client\n",
    "code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CbRsMsTyTkbo"
   },
   "outputs": [],
   "source": [
    "StrategyName = Literal[\"passthrough\", \"fixed\", \"summary\"]\n",
    "\n",
    "def _strategy_from_name(name: StrategyName, **kwargs) -> BaseMessageStrategy:\n",
    "    if name == \"passthrough\":\n",
    "        return PassthroughMessageStrategy()\n",
    "    if name == \"fixed\":\n",
    "        return FixedWindowMessageStrategy(**kwargs)\n",
    "    if name == \"summary\":\n",
    "        return SummarizingMessageStrategy(**kwargs)\n",
    "    raise ValueError(f\"Unknown strategy '{name}'\")\n",
    "\n",
    "\n",
    "class SimpleConversation:\n",
    "    \"\"\"\n",
    "    Self-contained conversation wrapper with pluggable message strategies:\n",
    "      - Create with:\n",
    "          conv = SimpleConversation(strategy_name=\"fixed\", strategy_kwargs={\"window\": 14})\n",
    "        or:\n",
    "          conv = SimpleConversation(strategy=SummarizingMessageStrategy(keep_last=12))\n",
    "      - conv.chat(\"...\") to run and display\n",
    "      - conv.print_history() to dump the backing history\n",
    "      - conv.to_dicts() to get [{'role': ..., 'content': ...}, ...] from backing\n",
    "      - conv.clear_history() to reset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-5-mini\",\n",
    "        system_prompt: str = \"You are a helpful assistant.\",\n",
    "        temperature: float = 0.3,\n",
    "        session_id: Optional[str] = None,\n",
    "        # Strategy selection\n",
    "        strategy: Optional[BaseMessageStrategy] = None,\n",
    "        strategy_name: StrategyName = \"passthrough\",\n",
    "        strategy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        # Optional: provide a custom factory if you manage sessions externally\n",
    "        strategy_factory: Optional[Callable[[], BaseMessageStrategy]] = None,\n",
    "    ):\n",
    "        # Prompt with explicit history placeholder\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt.strip()),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "\n",
    "        llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "        # Per-session strategy store\n",
    "        self._strategy_store: Dict[str, BaseMessageStrategy] = {}\n",
    "\n",
    "        # Build a factory that creates a fresh strategy per session\n",
    "        if strategy_factory is not None:\n",
    "            factory = strategy_factory\n",
    "        elif strategy is not None:\n",
    "            # Use the provided instance for all sessions in this object\n",
    "            factory = lambda: strategy\n",
    "        else:\n",
    "            kwargs = strategy_kwargs or {}\n",
    "            factory = lambda: _strategy_from_name(strategy_name, **kwargs)\n",
    "\n",
    "        def _history_for_session(sid: str) -> BaseMessageStrategy:\n",
    "            return self._strategy_store.setdefault(sid, factory())\n",
    "\n",
    "        # Runnable that pulls history from the strategy instance\n",
    "        self._runnable = RunnableWithMessageHistory(\n",
    "            prompt | llm,\n",
    "            _history_for_session,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        self._session_id = session_id or str(uuid4())\n",
    "\n",
    "    def invoke(self, prompt: str):\n",
    "        return self._runnable.invoke(\n",
    "            {\"input\": prompt},\n",
    "            config={\"configurable\": {\"session_id\": self._session_id}},\n",
    "        )\n",
    "\n",
    "    def chat(self, prompt: str):\n",
    "        display_markdown(\"**Human:** \", raw=True)\n",
    "        display_markdown(prompt, raw=True)\n",
    "        output = self.invoke(prompt)\n",
    "        display_markdown(f\"**AI:** \", raw=True)\n",
    "        display_markdown(output.content, raw=True)\n",
    "\n",
    "    # Convenience accessors operate on the backing history, not the policy view\n",
    "    def _strategy_obj(self) -> BaseMessageStrategy:\n",
    "        return self._strategy_store.setdefault(self._session_id, PassthroughMessageStrategy())\n",
    "\n",
    "    def get_history(self) -> List[BaseMessage]:\n",
    "        \"\"\"Full backing history objects, not policy-trimmed view.\"\"\"\n",
    "        return self._strategy_obj().backing_messages\n",
    "\n",
    "    def to_dicts(self) -> List[Dict[str, Any]]:\n",
    "        return [{\"role\": m.type, \"content\": m.content} for m in self.get_history()]\n",
    "\n",
    "    def print_history(self):\n",
    "        history = self.get_history()\n",
    "        if not history:\n",
    "            print(\"(no history)\")\n",
    "            return\n",
    "        for msg in history:\n",
    "            role = msg.type.capitalize()\n",
    "            print(f\"{role}: {msg.content}\")\n",
    "\n",
    "    def clear_history(self):\n",
    "        self._strategy_store[self._session_id] = type(self._strategy_obj())()  # new fresh instance\n",
    "\n",
    "    @property\n",
    "    def session_id(self) -> str:\n",
    "        return self._session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TndJMihCFpD"
   },
   "source": [
    "### Examples\n",
    "\n",
    "At its simplest, `SimpleConversation` works like a basic chatbot: every message\n",
    "is stored in memory and sent back to the model each turn. There is no windowing\n",
    "or summarization in this mode, so the conversation grows indefinitely. If the\n",
    "history becomes too large for the model’s context, LangChain will not trim it\n",
    "and an error is likely to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "EzjppDhuYYBU",
    "outputId": "a8f077a4-e78c-440a-8da2-b4144574874b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello I am Jeff"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hi Jeff — nice to meet you. How can I help today? (If you’d like, tell me what you’re working on or what you need — I can help with info, writing, code, planning, brainstorming, or something else.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Who am I"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "From our conversation I only know your name is Jeff. If you mean \"Who am I?\" in a deeper way, I can help — but I need you to tell me what you want:\n",
       "\n",
       "- Do you mean literally (confirm details I already know)?\n",
       "- Do you mean socially/role-based (e.g., family, job, interests)?\n",
       "- Do you mean psychologically/philosophically (values, purpose, identity)?\n",
       "- Do you want a short bio or LinkedIn-style summary I can write for you?\n",
       "\n",
       "If you want to explore identity now, try these quick prompts (answer any):\n",
       "\n",
       "1. What roles do you play (examples: parent, manager, student, friend)?\n",
       "2. What are 3 values that matter most to you?\n",
       "3. What energizes you or makes you lose track of time?\n",
       "4. What are your top skills or strengths?\n",
       "5. What do you want people to remember about you?\n",
       "\n",
       "If you give a few answers I’ll draft a concise \"Who I am\" statement or a short bio. Which option would you like?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv = SimpleConversation()\n",
    "\n",
    "conv.chat(\"Hello I am Jeff\")\n",
    "conv.chat(\"Who am I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIvwuJrJfb0u"
   },
   "source": [
    "### Testing Chat History Overflow\n",
    "\n",
    "`test_overflow` stress-tests history handling by stuffing the chat with filler\n",
    "messages (random dates) after teaching two important facts: the user's name and\n",
    "favorite color. Run it once with a buffered strategy (Passthrough or FixedWindow)\n",
    "and once with SummarizingMessageStrategy. The buffered approach is prone to\n",
    "forget early facts once the window rolls or the context gets large, while the\n",
    "summary approach periodically compresses older turns into a compact system note,\n",
    "so it is more likely to retain key details like name and favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Gtn2foORZfO3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def test_overflow(c):\n",
    "  c.chat(\"Okay, then let me introduce myself, my name is Jeff\")\n",
    "  c.chat(\"You have ONE JOB! Remember that my favorite color is blue.\")\n",
    "  c.chat(\"Do you remember my name?\")\n",
    "  c.chat(\"Do you remember my favorite color?\")\n",
    "\n",
    "  # Generate random dates from the last 20 years\n",
    "  today = datetime.today()\n",
    "  days_in_20_years = 20 * 365  # rough approximation, fine for demo\n",
    "  for i in range(20):\n",
    "      random_days = random.randint(0, days_in_20_years)\n",
    "      random_date = today - timedelta(days=random_days)\n",
    "      formatted_date = random_date.strftime(\"%B %d, %Y\")\n",
    "      c.chat(f\"Please remember this date #{i}: {formatted_date}. Just tell me OK if you got it.\")\n",
    "\n",
    "  c.chat(\"Do you remember my name?\")\n",
    "  c.chat(\"Do you remember my favorite color?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqR0JmrKOAnI"
   },
   "source": [
    "Now we can run `test_overflow` using the fixed-window strategy. In this mode,\n",
    "the conversation keeps only the most recent N messages and discards everything\n",
    "older. This prevents the context from growing too large, but it also means that\n",
    "earlier facts—like the user’s name or favorite color—quickly fall out of memory.\n",
    "As a result, when asked again at the end, the chatbot is likely to forget these\n",
    "details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HIXiyvDbkb0e",
    "outputId": "2475f72a-bfad-4827-8bbd-022acf38963a"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Okay, then let me introduce myself, my name is Jeff"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hi Jeff — nice to meet you. How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You have ONE JOB! Remember that my favorite color is blue."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Got it, Jeff — your favorite color is blue. I’ll remember that for this conversation. Would you like me to save it so I’ll remember across future chats?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my name?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Yes — your name is Jeff. I also remember that your favorite color is blue for this conversation. Would you like me to save those details for future chats?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my favorite color?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Yes — your favorite color is blue. Would you like me to save that so I remember it in future conversations?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #0: July 20, 2019. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #1: March 08, 2007. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #2: August 08, 2008. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #3: July 11, 2025. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #4: April 11, 2007. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #5: October 24, 2009. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #6: November 04, 2020. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #7: November 29, 2022. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #8: September 14, 2008. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #9: April 06, 2015. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #10: October 06, 2010. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #11: May 23, 2010. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #12: March 03, 2025. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #13: March 16, 2006. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #14: January 26, 2019. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #15: March 05, 2021. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #16: February 11, 2010. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #17: December 04, 2005. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #18: October 12, 2020. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #19: July 22, 2014. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my name?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I don't know your name — you haven't told me it. If you'd like, tell me now and I'll remember it for this conversation."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my favorite color?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I don't know your favorite color — you haven't told me. If you tell me now, I'll remember it for the rest of this conversation. (I can't retain personal details across separate chats unless a memory feature is available.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_fixed = SimpleConversation(strategy_name=\"fixed\", strategy_kwargs={\"window\": 14})\n",
    "\n",
    "test_overflow(conv_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR1glDyVgjzq"
   },
   "source": [
    "We can now display the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqzPqXiNfmIH",
    "outputId": "4b880202-2165-4de7-cf17-6dd239a372a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Please remember this date #15: March 05, 2021. Just tell me OK if you got it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='OK', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 285, 'total_tokens': 423, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C8Dbwyp8uj4eS2wSnALaWyOOeOshO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--458b87e0-db73-4467-8c2e-fef18ab04dd2-0', usage_metadata={'input_tokens': 285, 'output_tokens': 138, 'total_tokens': 423, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
       " HumanMessage(content='Please remember this date #16: February 11, 2010. Just tell me OK if you got it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='OK', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 285, 'total_tokens': 359, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C8Dc67la3Onu5lARdXQg5yMV3hJNw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0c6b5338-2007-4cf6-b8a8-2f3b047b8c8c-0', usage_metadata={'input_tokens': 285, 'output_tokens': 74, 'total_tokens': 359, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
       " HumanMessage(content='Please remember this date #17: December 04, 2005. Just tell me OK if you got it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='OK', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 285, 'total_tokens': 359, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C8Dc9ZhjJa0vEiWuIQkUfaMG4Tsfk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--53cd5beb-8c50-41d1-881f-366a5abebc48-0', usage_metadata={'input_tokens': 285, 'output_tokens': 74, 'total_tokens': 359, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
       " HumanMessage(content='Please remember this date #18: October 12, 2020. Just tell me OK if you got it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='OK', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 285, 'total_tokens': 423, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C8DcBSFTda1pFkUJRPPEc5Lp1NCzV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ee9467da-68ac-45bd-b6d6-8e302b948499-0', usage_metadata={'input_tokens': 285, 'output_tokens': 138, 'total_tokens': 423, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
       " HumanMessage(content='Please remember this date #19: July 22, 2014. Just tell me OK if you got it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='OK', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 285, 'total_tokens': 359, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C8DcErdgsbANzl014NSMAC4yfRpOb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9354b8c2-8179-4ae0-a242-798ce1dafd4f-0', usage_metadata={'input_tokens': 285, 'output_tokens': 74, 'total_tokens': 359, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
       " HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't know your name — you haven't told me it. If you'd like, tell me now and I'll remember it for this conversation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 228, 'prompt_tokens': 267, 'total_tokens': 495, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C8DcHICZe7eOVfs60iRBU9tkjiF6e', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1653c4d6-6436-4203-9e84-6194641668f6-0', usage_metadata={'input_tokens': 267, 'output_tokens': 228, 'total_tokens': 495, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}),\n",
       " HumanMessage(content='Do you remember my favorite color?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't know your favorite color — you haven't told me. If you tell me now, I'll remember it for the rest of this conversation. (I can't retain personal details across separate chats unless a memory feature is available.)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 373, 'prompt_tokens': 276, 'total_tokens': 649, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C8DcLDyAt2b846fvoQqxeZJAqxfHf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--059e7db6-c602-43ba-97c1-75878959b7ca-0', usage_metadata={'input_tokens': 276, 'output_tokens': 373, 'total_tokens': 649, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_fixed.get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0anwsZY5gmo8"
   },
   "source": [
    "Next we test the summarizing strategy and see that the important information is remembered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yaSo_mnQlRa9",
    "outputId": "934132c6-7a02-4a6f-b369-41fa9eb318e4"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Okay, then let me introduce myself, my name is Jeff"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeff — I’m here to help. What can I do for you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You have ONE JOB! Remember that my favorite color is blue."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Got it — your favorite color is blue. I’ll remember that for this conversation.\n",
       "\n",
       "If you want me to remember it across future chats, I can’t store persistent memory here unless you enable a “memory” feature or tell me at the start of new sessions. Want me to use “blue” in how I respond now (examples, themes, formatting)?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my name?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Yes — your name is Jeff. I’ll remember it for this conversation. Want me to use it when addressing you?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my favorite color?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Yes — your favorite color is blue. I’ll remember that for this conversation."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #0: November 07, 2015. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #1: October 29, 2022. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #2: July 26, 2009. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #3: May 10, 2011. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #4: February 17, 2015. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #5: November 07, 2015. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #6: January 20, 2024. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #7: November 01, 2005. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #8: January 17, 2023. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #9: August 18, 2022. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #10: March 04, 2022. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #11: March 12, 2023. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #12: November 05, 2015. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #13: March 05, 2006. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #14: September 19, 2011. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #15: July 19, 2015. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #16: August 30, 2017. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #17: July 20, 2021. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #18: August 25, 2020. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please remember this date #19: May 15, 2011. Just tell me OK if you got it."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my name?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Yes — your name is Jeff. I’ll remember it for this session. Would you like me to use your name when I address you?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you remember my favorite color?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Yes — your favorite color is blue. I’ll remember that for this session. Would you like me to use it when I address you or in examples?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strategy=SummarizingMessageStrategy(keep_last=8, trigger_len=16)\n",
    "conv_sum = SimpleConversation(strategy=strategy)\n",
    "\n",
    "test_overflow(conv_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MKvVu5fgw1j"
   },
   "source": [
    "We can view the internal summary that the object keeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "ZmVdDvwkmBT3",
    "outputId": "2c86bf5b-a60f-4e92-fba8-d02a48f707f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'- User: Jeff — identified and remembered for this session.\\n- Highest‑priority instruction: “You have ONE JOB!” — remember Jeff’s favorite color = blue for this session (confirmed).\\n- Saved dates (15 total), each acknowledged with “OK”: Nov 07, 2015; Oct 29, 2022; Jul 26, 2009; May 10, 2011; Feb 17, 2015; Jan 20, 2024; Nov 01, 2005; Jan 17, 2023; Aug 18, 2022; Mar 04, 2022; Mar 12, 2023; Nov 05, 2015; Mar 05, 2006; Sep 19, 2011; Jul 19, 2015.\\n- Assistant acknowledgments: each date‑add request was replied to with “OK.”\\n- Constraint: assistant retains these facts only for the current session and cannot persist them across future chats unless a persistent memory feature is enabled.\\n- Pending decision: user has not confirmed whether the assistant should actively use Jeff’s name or the favorite color (blue) in addressing, examples, themes, or formatting.\\n- No other outstanding tasks or requests.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.get_summary_text()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
