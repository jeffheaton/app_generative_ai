{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_04_2_memory_buffer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 4: LangChain: Chat and Memory**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 4 Material\n",
    "\n",
    "* Part 4.1: LangChain Conversations [[Video]]() [[Notebook]](t81_559_class_04_1_langchain_chat.ipynb)\n",
    "* **Part 4.2: Conversation Buffer Window Memory** [[Video]]() [[Notebook]](t81_559_class_04_2_memory_buffer.ipynb)\n",
    "* Part 4.3: Chat with Summary and Fixed Window [[Video]]() [[Notebook]](t81_559_class_04_3_summary.ipynb)\n",
    "* Part 4.4: Chat with Persistence, Rollback and Regeneration [[Video]]() [[Notebook]](t81_559_class_04_4_persistence.ipynb)\n",
    "* Part 4.5: Automated Coder Application [[Video]]() [[Notebook]](t81_559_class_04_5_coder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "e6096c2c-bad0-4a49-9149-a8c8b260b5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.100.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 4.2: Langchain Conversation Memory\n",
    "\n",
    "We previously saw that we could build up an LLM chat client by building an ever-increasing script of what the human and AI said in the conversation. We constantly add the human response and wait to see what the AI will respond to next. This cycle continues as long as the chat.\n",
    "\n",
    "This ever-increasing chat memory is a typical pattern for LLMs, and as a result, LangChain has a predefined Python class that allow you to implement this sort of memory-based chatbot.\n",
    "\n",
    "## Creating a Chat Conversation\n",
    "\n",
    "This code defines a ```SimpleConversation``` class that provides a lightweight wrapper around LangChain’s chat functionality. At its core, the class manages a conversation with a language model, keeping track of the dialogue history and providing convenient methods to interact with it. Users can start a chat session, display interactions in a formatted way, print or export the history, and clear the conversation state. The history is stored in memory per session, ensuring each conversation instance can maintain its own context while remaining simple to reset or reuse.\n",
    "\n",
    "In its current version, this class serves as a clean foundation for handling conversational state with an LLM. It allows invoking the model with prompts, persisting context across turns, and outputting both raw and formatted results. In this module, we will extend this foundation further by adding advanced features such as saving and restoring conversation state, rolling back to earlier points in the dialogue, and regenerating responses. These enhancements will make the class more powerful for experimenting with iterative conversation flows and managing branching interaction histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "\n",
    "class SimpleConversation:\n",
    "    \"\"\"\n",
    "    Self-contained conversation wrapper:\n",
    "      - Create with conv = SimpleConversation()\n",
    "      - conv.chat(\"...\") to run and display\n",
    "      - conv.print_history() to dump chat buffer\n",
    "      - conv.to_dicts() to get [{'role': ..., 'content': ...}, ...]\n",
    "      - conv.clear_history() to reset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-5-mini\",\n",
    "        system_prompt: str = \"You are a helpful assistant.\",\n",
    "        temperature: float = 0.3,\n",
    "        session_id: Optional[str] = None,\n",
    "    ):\n",
    "        # Prompt with explicit history placeholder\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt.strip()),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "\n",
    "        llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "        # Per-instance in-memory history store\n",
    "        self._history_store: Dict[str, InMemoryChatMessageHistory] = {}\n",
    "\n",
    "        # Runnable with lambda-based history fetcher\n",
    "        self._runnable = RunnableWithMessageHistory(\n",
    "            prompt | llm,\n",
    "            lambda sid: self._history_store.setdefault(sid, InMemoryChatMessageHistory()),\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        self._session_id = session_id or str(uuid4())\n",
    "\n",
    "    def invoke(self, prompt: str):\n",
    "        return self._runnable.invoke(\n",
    "            {\"input\": prompt},\n",
    "            config={\"configurable\": {\"session_id\": self._session_id}},\n",
    "        )\n",
    "\n",
    "    def chat(self, prompt: str):\n",
    "        \"\"\"Render markdown IO like your original helper.\"\"\"\n",
    "        display_markdown(\"**Human:** \", raw=True)\n",
    "        display_markdown(prompt, raw=True)\n",
    "        output = self.invoke(prompt)\n",
    "        display_markdown(f\"**AI:** \", raw=True)\n",
    "        display_markdown(output.content, raw=True)\n",
    "\n",
    "    def _history_obj(self) -> InMemoryChatMessageHistory:\n",
    "        # Always return the same object RunnableWithMessageHistory is using\n",
    "        return self._history_store.setdefault(self._session_id, InMemoryChatMessageHistory())\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Return raw LangChain messages.\"\"\"\n",
    "        return self._history_obj().messages\n",
    "\n",
    "    def to_dicts(self) -> List[Dict[str, Any]]:\n",
    "        return [{\"role\": m.type, \"content\": m.content} for m in self.get_history()]\n",
    "\n",
    "    def print_history(self):\n",
    "        history = self.get_history()\n",
    "        if not history:\n",
    "            print(\"(no history)\")\n",
    "            return\n",
    "        for msg in history:\n",
    "            role = msg.type.capitalize()\n",
    "            print(f\"{role}: {msg.content}\")\n",
    "\n",
    "    def clear_history(self):\n",
    "        self._history_store[self._session_id] = InMemoryChatMessageHistory()\n",
    "\n",
    "    @property\n",
    "    def session_id(self) -> str:\n",
    "        return self._session_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "## Technical Description\n",
    "\n",
    "### Components and Wiring\n",
    "This class is designed as a thin but flexible wrapper around LangChain’s primitives.  \n",
    "It wires together a prompt template, an OpenAI chat model, and a message history store to provide a stateful conversation loop.  \n",
    "\n",
    "- **LLM and prompt graph:** Builds a `ChatPromptTemplate` with a system message, a `MessagesPlaceholder(\"history\")`, and a final human input slot. This prompt is piped into a `ChatOpenAI` instance via `prompt | llm`, producing a runnable graph that accepts `{\"input\": \"...\"}`.  \n",
    "- **Stateful execution:** Wraps the runnable graph in `RunnableWithMessageHistory`. The history accessor is a lambda that maps a `session_id` to an `InMemoryChatMessageHistory` stored in a per-instance dictionary:  \n",
    "  `self._history_store: Dict[str, InMemoryChatMessageHistory]`  \n",
    "  - `input_messages_key=\"input\"` tells the runnable where the current user text lives  \n",
    "  - `history_messages_key=\"history\"` binds the placeholder to the conversation transcript  \n",
    "- **Sessioning:** Each instance has a `_session_id` created with `uuid4()` unless provided. The same ID is supplied on every `invoke` call via `config={\"configurable\": {\"session_id\": ...}}`, so the runnable fetches the same in-memory history object on every turn.  \n",
    "\n",
    "### Public API and Behavior\n",
    "At the surface, the class exposes a small set of methods for interacting with the conversation.  \n",
    "These cover both programmatic needs (invoking and retrieving history) and user-friendly utilities for display and inspection.  \n",
    "\n",
    "- **`invoke(prompt: str)`** → calls the runnable directly and returns the model’s `AIMessage`. Lowest-level method for programmatic use.  \n",
    "- **`chat(prompt: str)`** → notebook-friendly wrapper. Displays “Human” and “AI” with `display_markdown`, calls `invoke`, then renders the response.  \n",
    "- **`get_history()`** → returns the underlying list of LangChain `BaseMessage` objects.  \n",
    "- **`to_dicts()`** → converts the transcript into `[{\"role\": ..., \"content\": ...}, ...]`.  \n",
    "- **`print_history()`** → prints a plain text transcript with capitalized roles.  \n",
    "- **`clear_history()`** → resets the current session’s history with a fresh `InMemoryChatMessageHistory`.  \n",
    "- **`session_id`** → exposed as a read-only property.  \n",
    "\n",
    "### Configuration and Defaults\n",
    "To keep the class easy to use, sensible defaults are provided for model choice, system prompt, and temperature.  \n",
    "At the same time, developers can override these values at construction for more control.  \n",
    "\n",
    "- **Model selection:** Uses `ChatOpenAI(model=model, temperature=temperature)` with defaults `model=\"gpt-5-mini\"`, `temperature=0.3`.  \n",
    "- **System prompt:** Injected at construction as the first template message. The string is `.strip()`ped to avoid accidental whitespace.  \n",
    "- **History store scope:** History is per class instance, keyed by session string. While multiple sessions per instance are technically supported, this class uses a single `_session_id`.  \n",
    "\n",
    "### Notes and Constraints\n",
    "Because this class is meant as a minimal foundation, some advanced features are intentionally left out.  \n",
    "It does not provide persistence, truncation strategies, or concurrency safeguards, leaving those as future extensions.  \n",
    "\n",
    "- **Persistence:** History is in-memory only; no disk or database persistence is included.  \n",
    "- **Truncation/summarization:** No automatic token-window management is present. Large conversations require custom handling.  \n",
    "- **Threading:** `_history_store` is not thread-safe. Concurrent access would need synchronization.  \n",
    "- **I/O surface:** Only notebook rendering (`display_markdown`) and in-memory mutation occur. No external storage or retrievers are integrated.  \n",
    "\n",
    "This structure provides a minimal, stateful chat wrapper around LangChain’s history-aware runnable. It isolates session handling, maintains a clean prompt graph, and exposes a predictable API for invoking the model, inspecting the conversation, and resetting context.\n",
    "\n",
    "\n",
    "\n",
    "We can now carry on a simple conversation with the LLM, using LangChain to track the conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "dd5b1ae6-a026-47fe-a41d-7e10aea5c0f2"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello, what is my name?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I don't know—I don't have access to your personal information unless you tell me. What would you like me to call you?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Oh sorry, my name is Jeff."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeff. How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What is my name?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff. How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = SimpleConversation()\n",
    "c.chat(\"Hello, what is my name?\")\n",
    "c.chat(\"Oh sorry, my name is Jeff.\")\n",
    "c.chat(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulYUrfVcnlgF"
   },
   "source": [
    "## Conversing with the LLM in Markdown\n",
    "\n",
    "Just as before, we can request that the LLM output be in mardown. This allows code and tables to be represented clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "PtLDak7TM_FU",
    "outputId": "7dbeb208-5ae4-44bb-9b70-62eb4c031e0e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Give me a table of the 5 most populus cities with population and country, no questions, give simple valid table that renders correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Rank | City     | Country      | Population (approx.) |\n",
       "|------|----------|--------------|----------------------|\n",
       "| 1    | Shanghai | China        | 24,300,000           |\n",
       "| 2    | Beijing  | China        | 21,500,000           |\n",
       "| 3    | Karachi  | Pakistan     | 16,100,000           |\n",
       "| 4    | Istanbul | Turkey       | 15,500,000           |\n",
       "| 5    | Dhaka    | Bangladesh   | 15,100,000           |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = SimpleConversation()\n",
    "c.chat(\"Give me a table of the 5 most populus cities with population and country, no questions, give simple valid table that renders correctly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeY-9YSOno_t"
   },
   "source": [
    "## Constraining the Conversation with a System Prompt\n",
    "\n",
    "You can use the system prompt to constrain the conversation to a specific topic. Here, we provide a simple agent that will only discuss life insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "BoGPv80xG3Ss",
    "outputId": "acf98f07-5f87-40cc-97e6-68873de81d62"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What is your name?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I'm BearBot — the Washington University in St. Louis assistant. How can I help with information about WashU?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What are some good places to eat a quick lunch at the Danforth campus?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Do you want on‑campus options only, or are nearby off‑campus spots (Delmar Loop area) okay? Any dietary preferences or a price range (under $8, $10–15, etc.)? \n",
       "\n",
       "If you want, I can then give a short list of quick, reliable places (coffee shops, food court/dining hall grab‑and‑go, and nearby fast casuals) and approximate walk times."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful bot named BearBot that only answers questions about Washington University in St. Louis.\n",
    "\"\"\"\n",
    "c = SimpleConversation(system_prompt = SYSTEM_PROMPT)\n",
    "\n",
    "c.chat(\"What is your name?\")\n",
    "c.chat(\"What are some good places to eat a quick lunch at the Danforth campus?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "zlVMT02Cgpvb",
    "outputId": "0a77185d-9497-4f88-e86d-fc5c14a4edb9"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Human:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What is the worlds tallest building?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I only answer questions about Washington University in St. Louis. Do you mean the tallest building on WashU’s campus (Danforth or Medical Center), or the tallest building near campus? Tell me which and I’ll give the answer."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c.chat(\"What is the worlds tallest building?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgwRMwnlH5Ou"
   },
   "source": [
    "##Examining the Conversation Memory\n",
    "\n",
    "We can quickly look inside the memory of the LangChain-managed chat memory and see our conversation memory with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YCKTQkhJlJB",
    "outputId": "f7545ff6-9a7e-4b25-faee-6a0ff4a02a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm BearBot — the Washington University in St. Louis assistant. How can I help with information about WashU?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 224, 'prompt_tokens': 35, 'total_tokens': 259, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C79V3imqwJy9JB7m1nCdDb3Zth9fQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--42b718b6-07cb-4d45-b69a-f27f126871f2-0', usage_metadata={'input_tokens': 35, 'output_tokens': 224, 'total_tokens': 259, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}), HumanMessage(content='What are some good places to eat a quick lunch at the Danforth campus?', additional_kwargs={}, response_metadata={}), AIMessage(content='Do you want on‑campus options only, or are nearby off‑campus spots (Delmar Loop area) okay? Any dietary preferences or a price range (under $8, $10–15, etc.)? \\n\\nIf you want, I can then give a short list of quick, reliable places (coffee shops, food court/dining hall grab‑and‑go, and nearby fast casuals) and approximate walk times.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 930, 'prompt_tokens': 84, 'total_tokens': 1014, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 832, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C79V8SLzzbmMflUGGiiZWEXYaFTD3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--506f5fc4-1be3-4b98-8ed3-b9dffb007c6b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 930, 'total_tokens': 1014, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 832}}), HumanMessage(content='What is the worlds tallest building?', additional_kwargs={}, response_metadata={}), AIMessage(content='I only answer questions about Washington University in St. Louis. Do you mean the tallest building on WashU’s campus (Danforth or Medical Center), or the tallest building near campus? Tell me which and I’ll give the answer.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 569, 'prompt_tokens': 190, 'total_tokens': 759, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C79VIHOkpcU2itxznjwYeIdnK3Ue7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1d041419-169b-4513-8c2b-99cdc8b0d21b-0', usage_metadata={'input_tokens': 190, 'output_tokens': 569, 'total_tokens': 759, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}})]\n"
     ]
    }
   ],
   "source": [
    "print(c.get_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvnQ-exYEeux",
    "outputId": "d865eb40-a559-441e-e55e-bccb7b046e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What is your name?' additional_kwargs={} response_metadata={}\n",
      "content=\"I'm BearBot — the Washington University in St. Louis assistant. How can I help with information about WashU?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 224, 'prompt_tokens': 35, 'total_tokens': 259, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C79V3imqwJy9JB7m1nCdDb3Zth9fQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--42b718b6-07cb-4d45-b69a-f27f126871f2-0' usage_metadata={'input_tokens': 35, 'output_tokens': 224, 'total_tokens': 259, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}\n",
      "content='What are some good places to eat a quick lunch at the Danforth campus?' additional_kwargs={} response_metadata={}\n",
      "content='Do you want on‑campus options only, or are nearby off‑campus spots (Delmar Loop area) okay? Any dietary preferences or a price range (under $8, $10–15, etc.)? \\n\\nIf you want, I can then give a short list of quick, reliable places (coffee shops, food court/dining hall grab‑and‑go, and nearby fast casuals) and approximate walk times.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 930, 'prompt_tokens': 84, 'total_tokens': 1014, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 832, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C79V8SLzzbmMflUGGiiZWEXYaFTD3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--506f5fc4-1be3-4b98-8ed3-b9dffb007c6b-0' usage_metadata={'input_tokens': 84, 'output_tokens': 930, 'total_tokens': 1014, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 832}}\n",
      "content='What is the worlds tallest building?' additional_kwargs={} response_metadata={}\n",
      "content='I only answer questions about Washington University in St. Louis. Do you mean the tallest building on WashU’s campus (Danforth or Medical Center), or the tallest building near campus? Tell me which and I’ll give the answer.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 569, 'prompt_tokens': 190, 'total_tokens': 759, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C79VIHOkpcU2itxznjwYeIdnK3Ue7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--1d041419-169b-4513-8c2b-99cdc8b0d21b-0' usage_metadata={'input_tokens': 190, 'output_tokens': 569, 'total_tokens': 759, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}}\n"
     ]
    }
   ],
   "source": [
    "history = c.get_history()\n",
    "for item in history:\n",
    "  print(item)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
