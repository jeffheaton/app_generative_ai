{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_03_1_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 3: Large Language Models**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 3 Material\n",
    "\n",
    "* **Part 3.1: Foundation Models** [[Video]](https://www.youtube.com/watch?v=Gb0tk5qq1fA) [[Notebook]](t81_559_class_03_1_llm.ipynb)\n",
    "* Part 3.2: Text Generation [[Video]](https://www.youtube.com/watch?v=lB97Lqt7q58) [[Notebook]](t81_559_class_03_2_text_gen.ipynb)\n",
    "* Part 3.3: Text Summarization [[Video]](https://www.youtube.com/watch?v=3MoIUXE2eEU) [[Notebook]](t81_559_class_03_3_text_summary.ipynb)\n",
    "* Part 3.4: Text Classification [[Video]](https://www.youtube.com/watch?v=2VpOwFIGmA8) [[Notebook]](t81_559_class_03_4_classification.ipynb)\n",
    "* Part 3.5: LLM Writes a Book [[Video]](https://www.youtube.com/watch?v=iU40Rttlb_Q) [[Notebook]](t81_559_class_03_5_book.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "70f2f152-66ad-4aef-c27e-c81aab8c15cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.99.9)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 3.1: Foundation Models\n",
    "\n",
    "A foundation model for large language models (LLMs) refers to a base model that has been pre-trained on a broad range of data and can be adapted or fine-tuned for specific tasks or applications. These models are called \"foundation\" because they provide a foundational layer of knowledge and capabilities upon which specialized functionalities can be built.\n",
    "\n",
    "Several prominent technology companies and research organizations provide large language models. Notable among them are OpenAI with models like GPT (Generative Pre-trained Transformer), Google with BERT (Bidirectional Encoder Representations from Transformers) and other variants, and Facebook (Meta) which offers models such as RoBERTa (Robustly Optimized BERT Pretraining Approach).\n",
    "\n",
    "Training a large language model from scratch involves significant computational resources and expertise. It requires extensive data collection, cleaning, and processing, along with access to high-powered computing infrastructure capable of handling immense datasets. The financial cost of training such models can run into millions of dollars, making it prohibitive for most individuals and even many organizations. Given these requirements, developing a large language model from scratch is generally beyond the scope of an academic course. Instead, courses may focus on teaching how to use and fine-tune existing models to solve specific problems or conduct research.\n",
    "\n",
    "## How to Evaluate a Foundation Model\n",
    "\n",
    "Evaluating foundation models is crucial to understand their capabilities, limitations, and suitability for specific tasks. Accurate evaluation ensures that the models are safe, reliable, and effective in real-world applications. It also helps in identifying potential biases and errors that could impact their performance.\n",
    "\n",
    "### Open vs. Closed Weights:\n",
    "\n",
    "* **Open weights** refer to models where the trained parameters are publicly accessible. This transparency allows researchers and developers to understand the model's workings, replicate studies, and customize or improve the model further.\n",
    "* **Closed weights** are proprietary models with restricted access to their parameters. These are typically offered by companies as part of commercial products or services, where revealing the weights might compromise business interests or user privacy.\n",
    "\n",
    "### Number of Parameter Weights:\n",
    "\n",
    "The number of parameter weights in a model indicates its capacity or complexity. Parameter weights are typically expressed in millions (M), billions (B), or trillions (T). The higher the number of parameters, the greater the model’s theoretical ability to capture complex patterns and nuances in data. However, this is not a strict indicator of performance across all tasks but rather a general measure of potential.\n",
    "\n",
    "### Pros and Cons of More Weights:\n",
    "\n",
    "**Pros:**\n",
    "* **Increased Learning Capacity:** Models with more parameters can learn more detailed and nuanced representations of data, potentially improving their accuracy and effectiveness across diverse tasks.\n",
    "* **Better Generalization:** Larger models often generalize better to new, unseen datasets, provided they are trained appropriately.\n",
    "\n",
    "**Cons:**\n",
    "* **Computational Cost:** More parameters mean higher computational demands for training and inference, requiring more powerful hardware and longer processing times.\n",
    "* **Risk of Overfitting:** Without proper regularization and training techniques, larger models might overfit the training data, performing well on seen data but poorly on new or varied datasets.\n",
    "* **Environmental Impact:** Training larger models consumes more energy, contributing to larger carbon footprints.\n",
    "\n",
    "### Importance of Context Window Size:\n",
    "The context window size of a model refers to the maximum number of tokens (words or pieces of words) the model can consider at one time when making predictions or generating text. This is crucial because:\n",
    "\n",
    "* **Longer Context:** A larger window size allows the model to consider more information, which can lead to more coherent and contextually appropriate outputs. It's particularly important for tasks involving long documents or complex dependencies between parts of the text.\n",
    "* **Handling Dependencies:** The ability to handle long-range dependencies within the text can dramatically improve performance in tasks like summarization, question answering, and conversation.\n",
    "\n",
    "Understanding these aspects helps in making informed decisions about which model to use for a specific application and how to optimize its performance.\n",
    "\n",
    "The following table provides key statistics for many popular LLMs.\n",
    "\n",
    "### Understanding Tokens\n",
    "\n",
    "In the context of Large Language Models (LLMs) like GPT (Generative Pre-trained Transformer), tokens are the basic units of text that the model processes. A token can be a word, part of a word, or even punctuation. The definition of what exactly constitutes a token depends on the tokenizer used during the training of the model. For example, the word \"smiling\" might be a single token, or it could be split into smaller sub-word units like \"smile\" and \"ing\" depending on the tokenizer's vocabulary and rules.\n",
    "\n",
    "The cost of using a LLM for generating text is often calculated based on the number of tokens processed. This includes both the tokens that make up the input and the tokens generated as output. Because the computational resources required to process each token are significant, especially in models with billions of parameters, the number of tokens directly influences the computational cost. Thus, understanding how to efficiently manage token usage is crucial for optimizing expenses when using LLMs.\n",
    "\n",
    "The context window size of a LLM refers to the maximum number of tokens from the input that the model can consider at one time when making predictions. For example, if a model has a context window of 1,024 tokens, it can only consider the most recent 1,024 tokens of a given input. This limitation affects how much information the model can utilize at once. If the input exceeds this size, the model may not be able to refer back to earlier parts of the text, potentially impacting the coherence and relevance of its responses.\n",
    "\n",
    "The concept of converting tokens into \"pages\" is a useful metaphor for understanding how much content a model can handle or generate. While there's no universal standard for how many tokens equate to a \"page\" of text, a rough approximation is often used based on typical word counts per page in standard documents. For instance, if one page of text typically contains about 500 words, and assuming an average of 1.5 tokens per word, a page would roughly equate to 750 tokens.\n",
    "\n",
    "This approximation can help users gauge how much text they can input or expect in output in more familiar terms, like pages, which can be particularly helpful in educational, professional, or literary contexts where traditional page counts are more commonly used as a measure of content volume.\n",
    "\n",
    "## Common Large Language Models\n",
    "\n",
    "The following table provides key statistics for many popular LLMs. In this course, we will focus primarily on the OpenAI models, which are the first three rows of this table.\n",
    "\n",
    "| Model              | Creator    | Open?  | API Name(s) (if applicable)                              | Context Window                                                     |\n",
    "| ------------------ | ---------- | ------ | -------------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| **GPT-5**          | OpenAI     | Closed | `gpt-5`, `gpt-5-mini`, `gpt-5-nano` | 400K tokens (\\~296 pages)                                      |\n",
    "| **o4-mini**        | OpenAI     | Closed | —                                                        | \\~200K tokens (\\~148 pages)                                        |                                        |\n",
    "| **gpt-oss-120b**   | OpenAI     | Open   | —                      | 131K tokens (\\~97 pages)                                           |\n",
    "| **gpt-oss-20b**    | OpenAI     | Open   | —                     | 131K tokens (\\~97 pages)                                           |\n",
    "| **Claude 4**       | Anthropic  | Closed | —                                                        | 1M tokens (\\~741 pages)                                            |\n",
    "| **Gemini 2.5 Pro** | Google     | Closed | —                                                        | 1M tokens (\\~741 pages) |\n",
    "| **Llama 3.1 405B** | Meta       | Open   | —                                                        | 512K tokens (\\~379 pages)                                          |\n",
    "| **DeepSeek-V3**    | DeepSeek   | Open   | —                                                        | 128K tokens (\\~95 pages)                                           |\n",
    "| **Mistral 7B**     | Mistral.AI | Open   | —                                                        | 32K tokens (\\~24 pages)                                            |\n",
    "\n",
    "\n",
    "You will see that I estimate the number of pages, assuming 1,000 tokens would correspond to approximately 1.5 pages. This estimate assumes the same average of around 500 words per page and that each token corresponds to about 0.7314 words. The exact number might vary depending on formatting and content, but 1.5 pages per 1,000 tokens is a reasonable estimate.\n",
    "\n",
    "We also use the [MMLU benchmark](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) to measure how advanced the model is. Think of it as a sort of IQ of the models, higher is better.\n",
    "\n",
    "In this course, we will make use of the following OpenAI models, summarized here:\n",
    "\n",
    "* **gpt-5-mini** - The most cost effective model available from OpenAI, the vast majority of the assignments will make use of this model.\n",
    "* **gpt-5** - A somewhat more expensive model that we will use when we need more in-depth answers than mini provides.\n",
    "\n",
    "These different models have a variety of costs associated with them. I will provide guidelines for the most cost-effective model for each assignment. I present a summary of costs here.\n",
    "\n",
    "OpenAI GPT-5 API Pricing (per 1M tokens)\n",
    "\n",
    "| Model        | Input Cost | Output Cost | Notes                          |\n",
    "|--------------|------------|-------------|--------------------------------|\n",
    "| **GPT-5**        | $1.25       | $10.00       | Standard model for complex tasks |\n",
    "| **GPT-5 mini**   | $0.25       | $2.00        | Cheaper option for most coursework |\n",
    "| **GPT-5 nano**   | $0.05       | $0.40        | Fastest, lowest cost for simple processing |\n",
    "\n",
    "\n",
    "As you can see, in this course we will primarily use the **gpt-5-mini** model to keep costs under control. When longer input/output are needed, by virtue of the context window, we will use the more expensive **gpt-5**.\n",
    "\n",
    "When you specify a model string, such as `\"gpt-5\"`, you will use the latest GPT-5 model.  \n",
    "Specifying a specific model version, such as `\"gpt-5-2025-08-07\"`, is also possible.  \n",
    "\n",
    "For this course, I will always use the latest version (e.g., `\"gpt-5\"`, `\"gpt-5-mini\"`, or `\"gpt-5-nano\"`) and not specify a pinned version directly.  \n",
    "If you need the utmost control for reproducibility, you can specify precise versioned model strings instead.\n",
    "\n",
    "\n",
    "## Understanding Temperature\n",
    "\n",
    "\n",
    "OpenAI typically refers to its temperature setting in the context of AI models like ChatGPT. The temperature setting controls the randomness or creativity in the model's responses. A lower temperature (e.g., 0.0) results in more deterministic and predictable outputs, where the model strongly favors the most likely responses. On the other hand, a higher temperature (e.g., 1.0) increases randomness, leading to more varied and sometimes more creative outputs. The exact range can vary depending on the specific application or interface, but typically, it's set between 0 and 1. Adjusting the temperature allows users to tailor the balance between coherence and creativity in the model's responses to better suit specific tasks or preferences.\n",
    "\n",
    "The following code allows you to try out system prompts, temperatures, models, and requests.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = \"gpt-5-mini\"\n",
    "TEMPERATURE = 0.25\n",
    "\n",
    "SYSTEM = \"\"\"You are a friendly, truthful AI assistant.\n",
    "Format all answers in Markdown and provide accurate, concise explanations.\"\"\"\n",
    "\n",
    "# Build prompt with a slot for message history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Base LLM and chain\n",
    "llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)\n",
    "base_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Per-session memory store\n",
    "_store = {}\n",
    "def _get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in _store:\n",
    "        _store[session_id] = InMemoryChatMessageHistory()\n",
    "    return _store[session_id]\n",
    "\n",
    "# Wrap with message history\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    _get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "def start_conversation():\n",
    "    \"\"\"Return the runnable conversation directly (no tuple).\"\"\"\n",
    "    return conversation\n",
    "\n",
    "def query_llm(convo, user_input: str, session_id: str = \"default\"):\n",
    "    \"\"\"Invoke with LCEL + history and display Markdown.\"\"\"\n",
    "    result = convo.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"Model response:\")\n",
    "    display_markdown(result, raw=True)\n",
    "\n",
    "def reset_history(session_id: str = \"default\"):\n",
    "    \"\"\"Optional helper to clear a session's history.\"\"\"\n",
    "    if session_id in _store:\n",
    "        del _store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "WUPrAkqDW6GL",
    "outputId": "f674ba8d-3f3f-4cde-8882-00a249d85db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Country | Population (millions) | GDP (billions USD) |\n",
       "|---|---:|---:|\n",
       "| India | 1429 | 3732 |\n",
       "| China | 1425 | 19374 |\n",
       "| United States | 339 | 26854 |\n",
       "| Indonesia | 277 | 1404 |\n",
       "| Pakistan | 241 | 376 |\n",
       "\n",
       "Note: Values are approximate (most common recent estimates, ~2023)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = start_conversation()\n",
    "query_llm(conversation, \"\"\"\n",
    "Produce a table the top 5 most populous countries with population and GDP.\n",
    "Just use most common data, only numbers (billions for GDP, millions USD for GDP),\n",
    "simple table structure.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TjZs_TRht1n"
   },
   "source": [
    "# Module 3 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 3](https://github.com/jeffheaton/app_generative_ai/blob/main/assignments/assignment_yourname_t81_559_class3.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
