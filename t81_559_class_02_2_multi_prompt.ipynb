{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whjsJasuhstV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_02_2_multi_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOZxlIMhstX"
      },
      "source": [
        "# T81-559: Applications of Generative Artificial Intelligence\n",
        "**Module 2: Code Generation**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Yov72PhstY",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Module 2 Material\n",
        "\n",
        "* Part 2.1: Prompting for Code Generation [[Video]](https://www.youtube.com/watch?v=HVId6kYKKgQ) [[Notebook]](t81_559_class_02_1_dev.ipynb)\n",
        "* **Part 2.2: Handling Revision Prompts** [[Video]](https://www.youtube.com/watch?v=APpV46tplXA) [[Notebook]](t81_559_class_02_2_multi_prompt.ipynb)\n",
        "* Part 2.3: Using a LLM to Help Debug [[Video]](https://www.youtube.com/watch?v=VPqSNb38QK0) [[Notebook]](t81_559_class_02_3_llm_debug.ipynb)\n",
        "* Part 2.4: Tracking Prompts in Software Development [[Video]](https://www.youtube.com/watch?v=oUFUuYfvXZU) [[Notebook]](t81_559_class_02_4_software_eng.ipynb)\n",
        "* Part 2.5: Limits of LLM Code Generation [[Video]](https://www.youtube.com/watch?v=dKtRI0LZSyY) [[Notebook]](t81_559_class_02_5_code_gen_limits.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcAUP0c3hstY"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running and maps Google Drive if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsI496h5hstZ",
        "outputId": "a9542550-ae67-4f7f-8e02-53159af193df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.99.9)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.30\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# OpenAI Secrets\n",
        "if COLAB:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Install needed libraries in CoLab\n",
        "if COLAB:\n",
        "    !pip install langchain langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC9A-LaYhsta"
      },
      "source": [
        "# 2.2: Handling Revision Prompts\n",
        "\n",
        "Previously, we just sent one prompt to the LLM, which generated code. It is possible to perform this code more conversationally. In this module, we will see how to converse with the LLM to request changes to outputted code and even help the LLM to produce a more accurate model.\n",
        "\n",
        "We will also see that it might be beneficial to recreate your conversation as one single prompt that generates the final result. Keeping track of one prompt, rather than a conversation, that created your final code is more maintainable.\n",
        "\n",
        "## Conversational Code Generation\n",
        "\n",
        "We will introduce a more advanced code generation function that allows you to start the conversation to generate code and follow up with additional prompts if needed.\n",
        "\n",
        "In future modules, we will see how to create chatbots similar to this one. We will use the code I provided to generate your code for now. This generator uses a system prompt that requests that the generated code conform to the following:\n",
        "\n",
        "* Imports should be sorted\n",
        "* Code should conform to PEP-8 formatting\n",
        "* Do not mix uncompilable notes with code\n",
        "* Add comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TMF-rtxgRAea"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts.chat import PromptTemplate\n",
        "from IPython.display import display_markdown\n",
        "\n",
        "MODEL = 'gpt-5-mini'\n",
        "TEMPLATE = \"\"\"The following is a friendly conversation between a human and an\n",
        "AI to generate Python code. If you have notes about the code, place them before\n",
        "the code. Any nots about execution should follow the code. If you do mix any\n",
        "notes with the code, make them comments. Add proper comments to the code.\n",
        "Sort imports and follow PEP-8 formatting.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "Code Assistant:\"\"\"\n",
        "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
        "\n",
        "def start_conversation():\n",
        "    # Initialize the OpenAI LLM with your API key\n",
        "    llm = ChatOpenAI(\n",
        "        model=MODEL,\n",
        "        temperature=0.0,\n",
        "        n=1\n",
        "    )\n",
        "\n",
        "    # Initialize memory and conversation\n",
        "    memory = ConversationBufferWindowMemory()\n",
        "    conversation = ConversationChain(\n",
        "        prompt=PROMPT_TEMPLATE,\n",
        "        llm=llm,\n",
        "        memory=memory,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return conversation\n",
        "\n",
        "def generate_code(conversation, prompt):\n",
        "    print(\"Model response:\")\n",
        "    output = conversation.invoke(prompt)\n",
        "    display_markdown(output['response'], raw=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClPhLkGldhPt"
      },
      "source": [
        "## First Attempt at an XOR Approximator\n",
        "\n",
        "We will construct a prompt that requests the LLM to generate a PyTorch neural network to approximate the [Exclusive Or](https://en.wikipedia.org/wiki/Exclusive_or). The truth table for the Exclusive Or (XOR) function is provided here:\n",
        "\n",
        "```\n",
        "0 XOR 0 = 0\n",
        "1 XOR 0 = 1\n",
        "0 XOR 1 = 1\n",
        "1 XOR 1 = 0\n",
        "```\n",
        "\n",
        "If given data, neural networks can learn to approximate functions, so let's create a PyTorch neural network to approximate the XOR function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ydaqwgRiH4D6",
        "outputId": "f5a378e4-d50a-4a0a-ee70-83f2e0894a16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3012592608.py:29: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory()\n",
            "/tmp/ipython-input-3012592608.py:30: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Notes about the code:\n- This example trains a small feedforward network to learn XOR using PyTorch.\n- It uses a 2-neuron hidden layer with tanh activation and BCEWithLogitsLoss (stable sigmoid+BCELoss).\n- Matplotlib is used to optionally plot the decision boundary; it's not required to train.\n\n# Python code (save as xor_pytorch.py and run)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n# Small neural network for XOR: 2 -> 2 -> 1\nclass XORNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use a hidden layer with 2 units and tanh activation (classic for XOR)\n        self.net = nn.Sequential(\n            nn.Linear(2, 2),  # input to hidden\n            nn.Tanh(),        # nonlinearity\n            nn.Linear(2, 1)   # hidden to output (logit)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef main():\n    # Reproducibility\n    torch.manual_seed(0)\n    np.random.seed(0)\n\n    # Use GPU if available (not required)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # XOR dataset: four inputs and labels\n    inputs = torch.tensor([[0.0, 0.0],\n                           [0.0, 1.0],\n                           [1.0, 0.0],\n                           [1.0, 1.0]],\n                          device=device)\n    labels = torch.tensor([[0.0], [1.0], [1.0], [0.0]], device=device)\n\n    # Model, loss and optimizer\n    model = XORNet().to(device)\n    loss_fn = nn.BCEWithLogitsLoss()  # stable combination of sigmoid + BCELoss\n    optimizer = optim.Adam(model.parameters(), lr=0.1)\n\n    # Training loop\n    n_epochs = 5000\n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n\n        logits = model(inputs)               # raw scores (logits)\n        loss = loss_fn(logits, labels)       # compute loss\n        loss.backward()                      # backpropagate\n        optimizer.step()                     # update weights\n\n        # Print progress occasionally\n        if epoch % 500 == 0 or epoch == 1:\n            with torch.no_grad():\n                probs = torch.sigmoid(logits)\n                preds = (probs > 0.5).float()\n                accuracy = (preds == labels).float().mean().item()\n            print(f\"Epoch {epoch:4d}  Loss: {loss.item():.6f}  Accuracy: {accuracy * 100:.1f}%\")\n\n    # Evaluation on the 4 XOR inputs\n    model.eval()\n    with torch.no_grad():\n        logits = model(inputs)\n        probs = torch.sigmoid(logits).cpu().numpy().flatten()\n        preds = (probs > 0.5).astype(int)\n\n    print(\"\\nInputs | Probability | Pred\")\n    for inp, p, pr in zip(inputs.cpu().numpy(), probs, preds):\n        print(f\"{inp} -> {p:.4f} -> {pr}\")\n\n    # Optional: plot decision boundary (requires matplotlib)\n    try:\n        # Create a grid over input space\n        grid_x = np.linspace(-0.5, 1.5, 200)\n        grid_y = np.linspace(-0.5, 1.5, 200)\n        xx, yy = np.meshgrid(grid_x, grid_y)\n        grid = np.stack([xx.ravel(), yy.ravel()], axis=1).astype(np.float32)\n\n        # Evaluate model on grid\n        model.eval()\n        with torch.no_grad():\n            grid_t = torch.from_numpy(grid).to(device)\n            logits_grid = model(grid_t)\n            probs_grid = torch.sigmoid(logits_grid).cpu().numpy().reshape(xx.shape)\n\n        # Plot\n        plt.figure(figsize=(5, 4))\n        plt.contourf(xx, yy, probs_grid, levels=50, cmap=\"RdYlBu\", alpha=0.8)\n        plt.colorbar(label=\"P(y=1)\")\n        # Plot training points\n        xs = inputs.cpu().numpy()\n        ys = labels.cpu().numpy().flatten()\n        plt.scatter(xs[:, 0], xs[:, 1], c=ys, cmap=\"bwr\", edgecolor=\"k\", s=80)\n        plt.title(\"Learned XOR decision surface (probability)\")\n        plt.xlabel(\"x1\")\n        plt.ylabel(\"x2\")\n        plt.xlim(-0.5, 1.5)\n        plt.ylim(-0.5, 1.5)\n        plt.show()\n    except Exception:\n        # If plotting fails (e.g., matplotlib not available), skip silently\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n\nExecution notes:\n- Requires PyTorch. Install with pip if needed, e.g.:\n  - pip install torch torchvision    (or follow instructions at pytorch.org for the correct package for your platform)\n- To get the plotting output ensure matplotlib is installed: pip install matplotlib\n- Save the code to a file (e.g., xor_pytorch.py) and run: python xor_pytorch.py\n- Expected outcome: the network should reach ~100% accuracy on the four XOR inputs and the printed probabilities should be near 0 or 1. Adjust learning rate, optimizer, or epochs if training fails to converge."
          },
          "metadata": {}
        }
      ],
      "source": [
        "conversation = start_conversation()\n",
        "generate_code(conversation, \"\"\"Write Python code to learn the XOR function with PyTorch.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPqM7BslmM7"
      },
      "source": [
        "# Requesting a Change to Generated Code\n",
        "\n",
        "If you've taken my other course, you will know I prefer PyTorch sequences over extending the nn.Module class, at least for simple neural networks like an XOR approximator. LLMs do not share this opinion. However, the LLM will gladly humor me and generate a sequence. Here, I provide an additional prompt to request this rather than resubmitting a modified version of my first prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lqnDZhc4OVU6",
        "outputId": "2b8ce130-9899-4d1a-b1fa-1c531d33cbff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Notes about the code:\n- This example trains a small feedforward network to learn XOR using PyTorch.\n- It uses nn.Sequential (a PyTorch sequence) instead of defining a custom nn.Module subclass.\n- The network is 2 -> 2 -> 1 with Tanh activation in the hidden layer and BCEWithLogitsLoss.\n- Matplotlib is optionally used to plot the learned decision surface.\n\n# Python code (save as xor_pytorch_seq.py and run)\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n# Train a small network for XOR using nn.Sequential (no custom class)\ndef main():\n    # Reproducibility\n    torch.manual_seed(0)\n    np.random.seed(0)\n\n    # Use GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # XOR dataset: four inputs and labels (float tensors)\n    inputs = torch.tensor(\n        [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]],\n        dtype=torch.float32,\n        device=device,\n    )\n    labels = torch.tensor([[0.0], [1.0], [1.0], [0.0]],\n                          dtype=torch.float32,\n                          device=device)\n\n    # Build the model using nn.Sequential (2 -> 2 -> 1)\n    model = nn.Sequential(\n        nn.Linear(2, 2),  # input to hidden\n        nn.Tanh(),        # nonlinearity\n        nn.Linear(2, 1)   # hidden to output (logit)\n    ).to(device)\n\n    # Loss and optimizer\n    loss_fn = nn.BCEWithLogitsLoss()  # combines sigmoid + BCE in a numerically stable way\n    optimizer = optim.Adam(model.parameters(), lr=0.1)\n\n    # Training loop\n    n_epochs = 5000\n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n\n        logits = model(inputs)            # raw output (logits)\n        loss = loss_fn(logits, labels)    # compute loss\n        loss.backward()                   # backpropagate\n        optimizer.step()                  # update weights\n\n        # Print progress occasionally\n        if epoch % 500 == 0 or epoch == 1:\n            with torch.no_grad():\n                probs = torch.sigmoid(logits)\n                preds = (probs > 0.5).float()\n                accuracy = (preds == labels).float().mean().item()\n            print(f\"Epoch {epoch:4d}  Loss: {loss.item():.6f}  Accuracy: {accuracy * 100:.1f}%\")\n\n    # Evaluation on the 4 XOR inputs\n    model.eval()\n    with torch.no_grad():\n        logits = model(inputs)\n        probs = torch.sigmoid(logits).cpu().numpy().flatten()\n        preds = (probs > 0.5).astype(int)\n\n    print(\"\\nInputs | Probability | Pred\")\n    for inp, p, pr in zip(inputs.cpu().numpy(), probs, preds):\n        print(f\"{inp} -> {p:.4f} -> {pr}\")\n\n    # Optional: plot decision boundary (requires matplotlib)\n    try:\n        # Create a grid over input space\n        grid_x = np.linspace(-0.5, 1.5, 200)\n        grid_y = np.linspace(-0.5, 1.5, 200)\n        xx, yy = np.meshgrid(grid_x, grid_y)\n        grid = np.stack([xx.ravel(), yy.ravel()], axis=1).astype(np.float32)\n\n        # Evaluate model on the grid\n        model.eval()\n        with torch.no_grad():\n            grid_t = torch.from_numpy(grid).to(device)\n            logits_grid = model(grid_t)\n            probs_grid = torch.sigmoid(logits_grid).cpu().numpy().reshape(xx.shape)\n\n        # Plot probability surface and training points\n        plt.figure(figsize=(5, 4))\n        plt.contourf(xx, yy, probs_grid, levels=50, cmap=\"RdYlBu\", alpha=0.8)\n        plt.colorbar(label=\"P(y=1)\")\n        xs = inputs.cpu().numpy()\n        ys = labels.cpu().numpy().flatten()\n        plt.scatter(xs[:, 0], xs[:, 1], c=ys, cmap=\"bwr\", edgecolors=\"k\", s=80)\n        plt.title(\"Learned XOR decision surface (probability)\")\n        plt.xlabel(\"x1\")\n        plt.ylabel(\"x2\")\n        plt.xlim(-0.5, 1.5)\n        plt.ylim(-0.5, 1.5)\n        plt.show()\n    except Exception as exc:  # If plotting fails, report and continue\n        print(\"Plot skipped:\", exc)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nExecution notes:\n- Requires PyTorch. Install with pip if needed:\n  - pip install torch torchvision    (or follow instructions at pytorch.org for the correct package for your platform)\n- To see the plotting output ensure matplotlib is installed: pip install matplotlib\n- Save the code to a file (e.g., xor_pytorch_seq.py) and run: python xor_pytorch_seq.py\n- Expected outcome: the network should reach ~100% accuracy on the four XOR inputs and the printed probabilities should be near 0 or 1. Adjust learning rate, optimizer, or epochs if training fails to converge."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_code(conversation, \"\"\"Could you make use of a PyTorch sequence rather than a nn.Module class?\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7U1Bx3Wbje1"
      },
      "source": [
        "# Testing the Generated Code\n",
        "\n",
        "LLMs are not overachievers; they will implement the code you ask for and not provide much more. When we run the XOR approximator's first version, the results are only sometimes accurate, especially if we run the program multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdJNKLVhRcD3",
        "outputId": "d5469e79-e07e-4312-b838-45716cedac52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 0.2511032819747925\n",
            "Epoch 1000 Loss: 0.24947082996368408\n",
            "Epoch 2000 Loss: 0.2487175017595291\n",
            "Epoch 3000 Loss: 0.24651208519935608\n",
            "Epoch 4000 Loss: 0.24037021398544312\n",
            "Epoch 5000 Loss: 0.22664645314216614\n",
            "Epoch 6000 Loss: 0.2073187381029129\n",
            "Epoch 7000 Loss: 0.19236287474632263\n",
            "Epoch 8000 Loss: 0.18179592490196228\n",
            "Epoch 9000 Loss: 0.1654074341058731\n",
            "Predicted values:\n",
            "tensor([[0.1578],\n",
            "        [0.6766],\n",
            "        [0.6521],\n",
            "        [0.5166]])\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the XOR network using a sequential container\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Initialize the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training data for XOR\n",
        "data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "labels = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    pred = model(data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(pred, labels)\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch} Loss: {loss.item()}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    test_pred = model(data)\n",
        "    print(\"Predicted values:\")\n",
        "    print(test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY-NmeRAoPVr"
      },
      "source": [
        "If you receive an error or the output is not exactly what you like, it is effective to provide that output and any errors to the LLM. Here, we provide the output and ask the LLM if that seems correct. Sometimes, the LLM may insist that the output is correct, so you must \"debate\" the LLM, providing additional details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bzkCRwx_RxxI",
        "outputId": "fd5cb6cf-4ec5-452e-a76a-136ac0364196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Notes about the code:\n- Your reported outputs (~0.4–0.58) mean the network didn't converge to the XOR solution. This can happen with small networks and unlucky initial weights or optimizer settings.\n- Remedies included here:\n  - Use a slightly larger hidden layer (4 units) so it's easier to train.\n  - Reinitialize weights using Xavier/Glorot initialization (breaks symmetry).\n  - Run a few short restarts with different seeds if a run gets stuck.\n  - Lower the learning rate a bit and include early stopping when accuracy hits 100%.\n\n# Python code (save as xor_pytorch_seq_retry.py and run)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\ndef build_model(hidden_size: int = 4, device: torch.device = torch.device(\"cpu\")) -> nn.Sequential:\n    \"\"\"Create an nn.Sequential model for XOR (2 -> hidden_size -> 1)\n    and initialize linear layers with Xavier uniform initialization.\n    \"\"\"\n    model = nn.Sequential(\n        nn.Linear(2, hidden_size),  # input -> hidden\n        nn.Tanh(),                  # classic XOR hidden nonlinearity\n        nn.Linear(hidden_size, 1),  # hidden -> logit output\n    ).to(device)\n\n    # Xavier/Glorot initialization for better starting weights\n    for m in model.modules():\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n    return model\n\n\ndef train_model(\n    model: nn.Sequential,\n    inputs: torch.Tensor,\n    labels: torch.Tensor,\n    lr: float = 0.05,\n    n_epochs: int = 5000,\n) -> tuple[nn.Sequential, float]:\n    \"\"\"Train model and return (model, final_accuracy).\"\"\"\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n\n        logits = model(inputs)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Early stop if solved\n        if epoch % 200 == 0 or epoch == 1:\n            with torch.no_grad():\n                probs = torch.sigmoid(logits)\n                preds = (probs > 0.5).float()\n                accuracy = (preds == labels).float().mean().item()\n            if accuracy == 1.0:\n                # solved; break early\n                break\n    # final accuracy\n    with torch.no_grad():\n        logits = model(inputs)\n        probs = torch.sigmoid(logits)\n        preds = (probs > 0.5).float()\n        accuracy = (preds == labels).float().mean().item()\n    return model, accuracy\n\n\ndef main():\n    # Reproducibility base seed (we still try a few randomized restarts)\n    base_seed = 0\n    np.random.seed(base_seed)\n\n    # Choose device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # XOR dataset\n    inputs = torch.tensor(\n        [[0.0, 0.0],\n         [0.0, 1.0],\n         [1.0, 0.0],\n         [1.0, 1.0]],\n        dtype=torch.float32,\n        device=device,\n    )\n    labels = torch.tensor(\n        [[0.0], [1.0], [1.0], [0.0]],\n        dtype=torch.float32,\n        device=device,\n    )\n\n    # Training hyperparameters\n    attempts = 5           # number of random restarts\n    n_epochs = 5000        # epochs per attempt\n    lr = 0.05              # learning rate\n    hidden_size = 4        # hidden units (increase from 2 to make convergence easier)\n\n    final_model = None\n    final_accuracy = 0.0\n\n    # Try a few random restarts with different seeds if a run gets stuck\n    for attempt in range(1, attempts + 1):\n        seed = base_seed + attempt\n        torch.manual_seed(seed)\n\n        model = build_model(hidden_size=hidden_size, device=device)\n        model, accuracy = train_model(model, inputs, labels, lr=lr, n_epochs=n_epochs)\n\n        print(f\"Attempt {attempt}/{attempts}  Seed {seed}  Accuracy: {accuracy * 100:.1f}%\")\n\n        final_model = model\n        final_accuracy = accuracy\n        if accuracy == 1.0:\n            break  # solved\n\n    # Show final predictions (probabilities and binary predictions)\n    final_model.eval()\n    with torch.no_grad():\n        logits = final_model(inputs)\n        probs = torch.sigmoid(logits).cpu().numpy().flatten()\n        preds = (probs > 0.5).astype(int)\n\n    print(\"\\nInputs | Probability | Pred\")\n    for inp, p, pr in zip(inputs.cpu().numpy(), probs, preds):\n        print(f\"{inp} -> {p:.4f} -> {pr}\")\n\n    # Optional: plot decision boundary\n    try:\n        grid_x = np.linspace(-0.5, 1.5, 200)\n        grid_y = np.linspace(-0.5, 1.5, 200)\n        xx, yy = np.meshgrid(grid_x, grid_y)\n        grid = np.stack([xx.ravel(), yy.ravel()], axis=1).astype(np.float32)\n\n        final_model.eval()\n        with torch.no_grad():\n            grid_t = torch.from_numpy(grid).to(device)\n            logits_grid = final_model(grid_t)\n            probs_grid = torch.sigmoid(logits_grid).cpu().numpy().reshape(xx.shape)\n\n        plt.figure(figsize=(5, 4))\n        plt.contourf(xx, yy, probs_grid, levels=50, cmap=\"RdYlBu\", alpha=0.8)\n        plt.colorbar(label=\"P(y=1)\")\n        xs = inputs.cpu().numpy()\n        ys = labels.cpu().numpy().flatten()\n        plt.scatter(xs[:, 0], xs[:, 1], c=ys, cmap=\"bwr\", edgecolors=\"k\", s=80)\n        plt.title(\"Learned XOR decision surface (probability)\")\n        plt.xlabel(\"x1\")\n        plt.ylabel(\"x2\")\n        plt.xlim(-0.5, 1.5)\n        plt.ylim(-0.5, 1.5)\n        plt.show()\n    except Exception as exc:\n        print(\"Plot skipped:\", exc)\n\n\nif __name__ == \"__main__\":\n    main()\n\nExecution notes:\n- Requires PyTorch (pip install torch torchvision) and optionally matplotlib (pip install matplotlib).\n- Save as xor_pytorch_seq_retry.py and run: python xor_pytorch_seq_retry.py\n- If you still see probabilities near 0.5:\n  - Try increasing hidden_size (e.g., to 8), increasing epochs, or running more restarts.\n  - You can also lower the learning rate further (e.g., 0.01) or try SGD with momentum instead of Adam."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_code(conversation, \"\"\"The output was:\n",
        "\n",
        "Predicted values:\n",
        "tensor([[0.4843],\n",
        "        [0.5800],\n",
        "        [0.4278],\n",
        "        [0.4623]])\n",
        "\n",
        "Are you sure that is correct?\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r2Qpe11cWwm"
      },
      "source": [
        "## Test the Improved Version\n",
        "\n",
        "We now receive much more accurate output when we test the neural network provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V19QyaROSUP1",
        "outputId": "bf7fdc8b-4ff1-4bb5-8368-db0a28255812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 0.2703022062778473\n",
            "Epoch 1000 Loss: 7.112702587619424e-05\n",
            "Epoch 2000 Loss: 2.0544111976050772e-05\n",
            "Epoch 3000 Loss: 8.988646186480764e-06\n",
            "Epoch 4000 Loss: 4.589944182953332e-06\n",
            "Epoch 5000 Loss: 2.524477849874529e-06\n",
            "Epoch 6000 Loss: 1.4446948171098484e-06\n",
            "Epoch 7000 Loss: 8.453595228274935e-07\n",
            "Epoch 8000 Loss: 5.010292056795151e-07\n",
            "Epoch 9000 Loss: 2.991056078371912e-07\n",
            "Epoch 10000 Loss: 1.7932310925061756e-07\n",
            "Epoch 11000 Loss: 1.0772929925906283e-07\n",
            "Epoch 12000 Loss: 6.481739944774745e-08\n",
            "Epoch 13000 Loss: 3.9054452116715765e-08\n",
            "Epoch 14000 Loss: 2.3564556528299363e-08\n",
            "Epoch 15000 Loss: 1.4229989631076023e-08\n",
            "Epoch 16000 Loss: 8.607841550656303e-09\n",
            "Epoch 17000 Loss: 5.218104170978677e-09\n",
            "Epoch 18000 Loss: 3.1716662629577286e-09\n",
            "Epoch 19000 Loss: 1.9367694115146605e-09\n",
            "Predicted values:\n",
            "tensor([[1.5428e-05],\n",
            "        [9.9997e-01],\n",
            "        [9.9997e-01],\n",
            "        [5.1830e-05]])\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the XOR network using a sequential container\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),  # Increased the number of neurons in the hidden layer\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(4, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Initialize the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)  # Changed to Adam optimizer\n",
        "\n",
        "# Training data for XOR\n",
        "data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "labels = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(20000):  # Increased the number of epochs\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    pred = model(data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(pred, labels)\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch} Loss: {loss.item()}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    test_pred = model(data)\n",
        "    print(\"Predicted values:\")\n",
        "    print(test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCAmOVHEpAjf"
      },
      "source": [
        "## Combining the Conversation into a Single Prompt\n",
        "\n",
        "We should combine this entire conversation into a single prompt, especially if we wish to save the prompt along with the code. We can request the LLM to create this combined prompt for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "MKMcmXTNSiFw",
        "outputId": "f2f2a97f-a909-4588-f8bf-24faf1f7b285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Write a single Python script (save as xor_pytorch_seq_retry.py) that trains a small feedforward network to learn XOR using PyTorch. Follow these exact requirements:\n\n- Place brief notes about the code before the code block and place execution notes after the code block. If any notes must be mixed with the code, make them Python comments.\n- Do NOT surround the code with Markdown fences in the output.\n- Sort imports alphabetically and follow PEP 8 formatting.\n- Use nn.Sequential (no custom nn.Module subclass) to build the model. The model should be 2 -> hidden_size -> 1, where hidden_size defaults to 4.\n- Use nn.Tanh for the hidden activation and use BCEWithLogitsLoss for the loss (i.e., logits + stable BCE).\n- Initialize all nn.Linear layers using Xavier/Glorot uniform initialization and zero biases.\n- Provide these functions with type hints and clear comments:\n  - build_model(hidden_size: int = 4, device: torch.device = torch.device(\"cpu\")) -> nn.Sequential\n  - train_model(model: nn.Sequential, inputs: torch.Tensor, labels: torch.Tensor, lr: float = 0.05, n_epochs: int = 5000) -> tuple[nn.Sequential, float]\n  - main()\n- In main():\n  - Use a base_seed and attempt multiple random restarts (attempts = 5). For each attempt set torch.manual_seed(base_seed + attempt).\n  - Create the XOR dataset as 4 float32 input vectors and labels.\n  - For each attempt, build the model, train it, and print: Attempt i/N  Seed S  Accuracy: XX.X%\n  - Stop early for an attempt if accuracy reaches 100% (early stopping).\n  - After all attempts or when solved, print the final 4 inputs with probabilities and binary predictions.\n- Training details:\n  - Use Adam optimizer with lr default 0.05.\n  - Train up to n_epochs per attempt (default 5000) but break early if solved.\n  - During training occasionally check for accuracy (e.g., every 200 epochs) and break when accuracy == 1.0.\n- Add an optional decision-boundary plot using matplotlib:\n  - Evaluate the model on a grid in [-0.5, 1.5]^2 and contourf the probability surface.\n  - Plot the 4 training points on top.\n  - Wrap plotting in try/except and print \"Plot skipped: <exception>\" if it fails.\n- After the code, include execution notes that mention how to install PyTorch and matplotlib (pip install torch torchvision, pip install matplotlib), how to run the script, and suggestions if the network still outputs probabilities near 0.5 (e.g., increase hidden_size, more restarts, lower lr, change optimizer).\n- Use clear inline comments in the code and keep the code concise and readable.\n\nProduce only the prompt above (no additional explanation)."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_code(conversation, \"\"\"Okay, that is great, can you suggest a single\n",
        "prompt that would have resulted in this last code output?\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw-QDu7Rpo_T"
      },
      "source": [
        "The LLM's attempt at a consoldated prompt is incomplete. It skips several important details and does not provide precise requirements. I will manually make some improvements, which you can see here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1TzyiT6aS7ej",
        "outputId": "134358dc-f5c7-4c3f-c6f9-4f221f3c2477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Notes:\n- The script trains a small feedforward neural network (nn.Sequential) to learn the XOR function.\n- Network: Linear(2 -> 4) -> Sigmoid -> Linear(4 -> 1) -> Sigmoid (4 hidden neurons).\n- Optimizer: Adam, 20_000 epochs, binary cross-entropy loss (BCELoss).\n- A random seed is set for reproducibility.\n\n```python\n# Train a small feedforward network (nn.Sequential) to learn XOR using PyTorch.\n# Network: 2 inputs -> 4 hidden neurons (sigmoid) -> 1 output (sigmoid).\n# Optimizer: Adam. Loss: BCELoss. Training epochs: 20_000.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Reproducibility\ntorch.manual_seed(0)\n\n# Device (using CPU here)\ndevice = torch.device(\"cpu\")\n\n# XOR dataset: inputs and targets\n# Inputs shape: (4, 2), Targets shape: (4, 1)\ninputs = torch.tensor(\n    [[0.0, 0.0],\n     [0.0, 1.0],\n     [1.0, 0.0],\n     [1.0, 1.0]],\n    dtype=torch.float32,\n    device=device,\n)\ntargets = torch.tensor(\n    [[0.0], [1.0], [1.0], [0.0]],\n    dtype=torch.float32,\n    device=device,\n)\n\n# Define the model using nn.Sequential (sequence, not a subclass of nn.Module)\nmodel = nn.Sequential(\n    nn.Linear(2, 4),  # input -> 4 hidden neurons\n    nn.Sigmoid(),     # activation for hidden layer\n    nn.Linear(4, 1),  # hidden -> output\n    nn.Sigmoid(),     # output activation (probability)\n).to(device)\n\n# Loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Training loop\nnum_epochs = 20_000\nprint_every = 2_000\n\nfor epoch in range(1, num_epochs + 1):\n    # Forward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward pass and optimization step\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Periodically print progress\n    if epoch % print_every == 0 or epoch == 1:\n        print(f\"Epoch {epoch:5d}/{num_epochs} - Loss: {loss.item():.6f}\")\n\n# Evaluate trained model\nwith torch.no_grad():\n    final_outputs = model(inputs)\n    predicted_probs = final_outputs.view(-1)  # flatten to (4,)\n    predicted_labels = (predicted_probs >= 0.5).long()\n    targets_long = targets.view(-1).long()\n    accuracy = (predicted_labels == targets_long).float().mean().item()\n\n    print(\"\\nFinal outputs (probabilities):\")\n    for inp, prob, lbl in zip(inputs, predicted_probs, predicted_labels):\n        print(f\"  Input: {inp.tolist()} -> Prob: {prob:.4f}  Pred: {int(lbl.item())}\")\n\n    print(f\"\\nAccuracy on XOR dataset: {accuracy * 100:.1f}%\")\n```\n\nExecution notes:\n- Requires PyTorch to be installed (pip install torch).\n- Training on CPU for 20k epochs is quick for this tiny model (a few seconds).\n- If you want faster convergence or different behavior, try changing the learning rate (lr) or using nn.Tanh activations. If you prefer to avoid explicit Sigmoid at the output, use nn.BCEWithLogitsLoss and remove the final Sigmoid."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Start a new conversation\n",
        "conversation = start_conversation()\n",
        "generate_code(conversation, \"\"\"\n",
        "Can you provide Python code using PyTorch to effectively learn the XOR function\n",
        "with 4 hidden neurons, using the Adam optimizer, and 20K training epochs?\n",
        "Use a sequence not a nn.Module class.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EiMd01ATosU"
      },
      "source": [
        "## Test the Final Prompt\n",
        "\n",
        "Now, we test the final prompt. My prompt produces an acceptable result, but there are some opportunities for improvement. You can specify the exact format for the output. For example, sometimes code is generated to round the results, but other times it is not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx0_Al1kTmh7",
        "outputId": "702d42a9-0f3a-4641-f701-e0aa09e90265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1000/20000], Loss: 0.0034\n",
            "Epoch [2000/20000], Loss: 0.0008\n",
            "Epoch [3000/20000], Loss: 0.0003\n",
            "Epoch [4000/20000], Loss: 0.0001\n",
            "Epoch [5000/20000], Loss: 0.0001\n",
            "Epoch [6000/20000], Loss: 0.0000\n",
            "Epoch [7000/20000], Loss: 0.0000\n",
            "Epoch [8000/20000], Loss: 0.0000\n",
            "Epoch [9000/20000], Loss: 0.0000\n",
            "Epoch [10000/20000], Loss: 0.0000\n",
            "Epoch [11000/20000], Loss: 0.0000\n",
            "Epoch [12000/20000], Loss: 0.0000\n",
            "Epoch [13000/20000], Loss: 0.0000\n",
            "Epoch [14000/20000], Loss: 0.0000\n",
            "Epoch [15000/20000], Loss: 0.0000\n",
            "Epoch [16000/20000], Loss: 0.0000\n",
            "Epoch [17000/20000], Loss: 0.0000\n",
            "Epoch [18000/20000], Loss: 0.0000\n",
            "Epoch [19000/20000], Loss: 0.0000\n",
            "Epoch [20000/20000], Loss: 0.0000\n",
            "Predicted tensor: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Actual tensor: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the XOR inputs and outputs\n",
        "inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float)\n",
        "targets = torch.tensor([[0], [1], [1], [0]], dtype=torch.float)\n",
        "\n",
        "# Define the model using a sequential container\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),  # Input layer to hidden layer with 4 neurons\n",
        "    nn.ReLU(),        # ReLU activation function\n",
        "    nn.Linear(4, 1),  # Hidden layer to output layer\n",
        "    nn.Sigmoid()      # Sigmoid activation function for binary output\n",
        ")\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with learning rate of 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20000):  # 20,000 training epochs\n",
        "    optimizer.zero_grad()   # Clear gradients for each training step\n",
        "    outputs = model(inputs)  # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "    loss = criterion(outputs, targets)  # Compute loss\n",
        "    loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "    optimizer.step()  # Perform a single optimization step (parameter update)\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/20000], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing the model\n",
        "with torch.no_grad():  # Context-manager that disabled gradient calculation\n",
        "    predicted = model(inputs).round()  # Forward pass and rounding off to get predictions\n",
        "    print(f'Predicted tensor: {predicted}')\n",
        "    print(f'Actual tensor: {targets}')"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (genai)",
      "language": "python",
      "name": "genai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}